# SCIP version 3.1.0.1

# branching score function ('s'um, 'p'roduct, 'q'uotient)
# [type: char, range: {spq}, default: p]
branching/scorefunc = p

# branching score factor to weigh downward and upward gain prediction in sum score function
# [type: real, range: [0,1], default: 0.167]
branching/scorefac = 0.167

# should branching on binary variables be preferred?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
branching/preferbinary = FALSE

# minimal relative distance of branching point to bounds when branching on a continuous variable
# [type: real, range: [0,0.5], default: 0.2]
branching/clamp = 0.2

# strategy for normalization of LP gain when updating pseudocosts of continuous variables (divide by movement of 'l'p value, reduction in 'd'omain width, or reduction in domain width of 's'ibling)
# [type: char, range: {dls}, default: s]
branching/lpgainnormalize = s

# should updating pseudo costs for continuous variables be delayed to the time after separation?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
branching/delaypscostupdate = TRUE

# should all strong branching children be regarded even if one is detected to be infeasible? (only with propagation)
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
branching/forceallchildren = FALSE

# child node to be regarded first during strong branching (only with propagation): 'u'p child, 'd'own child, or 'a'utomatic
# [type: char, range: {adu}, default: a]
branching/firstsbchild = a

# should LP solutions during strong branching with propagation be checked for feasibility?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
branching/checksol = TRUE

# should LP solutions during strong branching with propagation be rounded? (only when checksbsol=TRUE)
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
branching/roundsbsol = TRUE

# should conflict analysis be enabled?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
conflict/enable = TRUE

# should propagation conflict analysis be used?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
conflict/useprop = TRUE

# should infeasible LP conflict analysis be used?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
conflict/useinflp = TRUE

# should bound exceeding LP conflict analysis be used?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
conflict/useboundlp = FALSE

# should infeasible/bound exceeding strong branching conflict analysis be used?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
conflict/usesb = FALSE

# should pseudo solution conflict analysis be used?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
conflict/usepseudo = TRUE

# maximal fraction of variables involved in a conflict constraint
# [type: real, range: [0,1.79769313486232e+308], default: 0.1]
conflict/maxvarsfac = 0.1

# minimal absolute maximum of variables involved in a conflict constraint
# [type: int, range: [0,2147483647], default: 30]
conflict/minmaxvars = 30

# maximal number of LP resolving loops during conflict analysis (-1: no limit)
# [type: int, range: [-1,2147483647], default: 2]
conflict/maxlploops = 2

# maximal number of LP iterations in each LP resolving loop (-1: no limit)
# [type: int, range: [-1,2147483647], default: 10]
conflict/lpiterations = 10

# number of depth levels up to which first UIP's are used in conflict analysis (-1: use All-FirstUIP rule)
# [type: int, range: [-1,2147483647], default: -1]
conflict/fuiplevels = -1

# maximal number of intermediate conflict constraints generated in conflict graph (-1: use every intermediate constraint)
# [type: int, range: [-1,2147483647], default: -1]
conflict/interconss = -1

# number of depth levels up to which UIP reconvergence constraints are generated (-1: generate reconvergence constraints in all depth levels)
# [type: int, range: [-1,2147483647], default: -1]
conflict/reconvlevels = -1

# maximal number of conflict constraints accepted at an infeasible node (-1: use all generated conflict constraints)
# [type: int, range: [-1,2147483647], default: 10]
conflict/maxconss = 10

# should binary conflicts be preferred?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
conflict/preferbinary = FALSE

# should conflict constraints be generated that are only valid locally?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
conflict/allowlocal = TRUE

# should conflict constraints be attached only to the local subtree where they can be useful?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
conflict/settlelocal = FALSE

# should earlier nodes be repropagated in order to replace branching decisions by deductions?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
conflict/repropagate = TRUE

# should constraints be kept for repropagation even if they are too long?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
conflict/keepreprop = TRUE

# should the conflict constraints be separated?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
conflict/separate = TRUE

# should the conflict constraints be subject to aging?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
conflict/dynamic = TRUE

# should the conflict's relaxations be subject to LP aging and cleanup?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
conflict/removable = TRUE

# score factor for depth level in bound relaxation heuristic of LP analysis
# [type: real, range: [-1.79769313486232e+308,1.79769313486232e+308], default: 1]
conflict/depthscorefac = 1

# factor to decrease importance of variables' earlier conflict scores
# [type: real, range: [1e-06,1], default: 0.98]
conflict/scorefac = 0.98

# number of successful conflict analysis calls that trigger a restart (0: disable conflict restarts)
# [type: int, range: [0,2147483647], default: 0]
conflict/restartnum = 0

# factor to increase restartnum with after each restart
# [type: real, range: [0,1.79769313486232e+308], default: 1.5]
conflict/restartfac = 1.5

# should relaxed bounds be ignored?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
conflict/ignorerelaxedbd = FALSE

# maximal number of variables to try to detect global bound implications and shorten the whole conflict set (0: disabled)
# [type: int, range: [0,2147483647], default: 250]
conflict/maxvarsdetectimpliedbounds = 250

# try to shorten the whole conflict set or terminate early (depending on the 'maxvarsdetectimpliedbounds' parameter)
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
conflict/fullshortenconflict = TRUE

# the weight the VSIDS score is weight by updating the VSIDS for a variable if it is part of a conflict
# [type: real, range: [0,1], default: 0]
conflict/conflictweight = 0

# the weight the VSIDS score is weight by updating the VSIDS for a variable if it is part of a conflict graph
# [type: real, range: [0,1], default: 1]
conflict/conflictgraphweight = 1

# maximum age an unnecessary constraint can reach before it is deleted (0: dynamic, -1: keep all constraints)
# [type: int, range: [-1,2147483647], default: 0]
constraints/agelimit = 0

# age of a constraint after which it is marked obsolete (0: dynamic, -1 do not mark constraints obsolete)
# [type: int, range: [-1,2147483647], default: -1]
constraints/obsoleteage = -1

# should enforcement of pseudo solution be disabled?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/disableenfops = FALSE

# verbosity level of output
# [type: int, range: [0,5], default: 4]
display/verblevel = 4

# maximal number of characters in a node information line
# [type: int, range: [0,2147483647], default: 139]
display/width = 139

# frequency for displaying node information lines
# [type: int, range: [-1,2147483647], default: 100]
display/freq = 100

# frequency for displaying header lines (every n'th node information line)
# [type: int, range: [-1,2147483647], default: 15]
display/headerfreq = 15

# should the LP solver display status messages?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
display/lpinfo = FALSE

# should statistics be collected for variable domain value pairs?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
history/valuebased = FALSE

# maximal time in seconds to run
# [type: real, range: [0,1.79769313486232e+308], default: 1e+20]
limits/time = 1e+20

# maximal number of nodes to process (-1: no limit)
# [type: longint, range: [-1,9223372036854775807], default: -1]
limits/nodes = -1

# maximal number of total nodes (incl. restarts) to process (-1: no limit)
# [type: longint, range: [-1,9223372036854775807], default: -1]
limits/totalnodes = -1

# solving stops, if the given number of nodes was processed since the last improvement of the primal solution value (-1: no limit)
# [type: longint, range: [-1,9223372036854775807], default: -1]
limits/stallnodes = -1

# maximal memory usage in MB; reported memory usage is lower than real memory usage!
# [type: real, range: [0,1.79769313486232e+308], default: 1e+20]
limits/memory = 1e+20

# solving stops, if the relative gap = |primal - dual|/MIN(|dual|,|primal|) is below the given value
# [type: real, range: [0,1.79769313486232e+308], default: 0]
limits/gap = 0

# solving stops, if the absolute gap = |primalbound - dualbound| is below the given value
# [type: real, range: [0,1.79769313486232e+308], default: 0]
limits/absgap = 0

# solving stops, if the given number of solutions were found (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
limits/solutions = -1

# solving stops, if the given number of solution improvements were found (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
limits/bestsol = -1

# maximal number of solutions to store in the solution storage
# [type: int, range: [1,2147483647], default: 100]
limits/maxsol = 100

# maximal number of solutions candidates to store in the solution storage of the original problem
# [type: int, range: [0,2147483647], default: 10]
limits/maxorigsol = 10

# solving stops, if the given number of restarts was triggered (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
limits/restarts = -1

# frequency for solving LP at the nodes (-1: never; 0: only root LP)
# [type: int, range: [-1,2147483647], default: 1]
lp/solvefreq = 1

# iteration limit for each single LP solve (-1: no limit)
# [type: longint, range: [-1,9223372036854775807], default: -1]
lp/iterlim = -1

# iteration limit for initial root LP solve (-1: no limit)
# [type: longint, range: [-1,9223372036854775807], default: -1]
lp/rootiterlim = -1

# maximal depth for solving LP at the nodes (-1: no depth limit)
# [type: int, range: [-1,2147483647], default: -1]
lp/solvedepth = -1

# LP algorithm for solving initial LP relaxations (automatic 's'implex, 'p'rimal simplex, 'd'ual simplex, 'b'arrier, barrier with 'c'rossover)
# [type: char, range: {spdbc}, default: s]
lp/initalgorithm = s

# LP algorithm for resolving LP relaxations if a starting basis exists (automatic 's'implex, 'p'rimal simplex, 'd'ual simplex, 'b'arrier, barrier with 'c'rossover)
# [type: char, range: {spdbc}, default: s]
lp/resolvealgorithm = s

# LP pricing strategy ('l'pi default, 'a'uto, 'f'ull pricing, 'p'artial, 's'teepest edge pricing, 'q'uickstart steepest edge pricing, 'd'evex pricing)
# [type: char, range: {lafpsqd}, default: l]
lp/pricing = l

# should lp state be cleared at the end of probing mode when lp was initially unsolved, e.g., when called right after presolving?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
lp/clearinitialprobinglp = TRUE

# should the LP be resolved to restore the state at start of diving (if FALSE we buffer the solution values)?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
lp/resolverestore = FALSE

# should the buffers for storing LP solution values during diving be freed at end of diving?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
lp/freesolvalbuffers = FALSE

# maximum age a dynamic column can reach before it is deleted from the LP (-1: don't delete columns due to aging)
# [type: int, range: [-1,2147483647], default: 10]
lp/colagelimit = 10

# maximum age a dynamic row can reach before it is deleted from the LP (-1: don't delete rows due to aging)
# [type: int, range: [-1,2147483647], default: 10]
lp/rowagelimit = 10

# should new non-basic columns be removed after LP solving?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
lp/cleanupcols = FALSE

# should new non-basic columns be removed after root LP solving?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
lp/cleanupcolsroot = FALSE

# should new basic rows be removed after LP solving?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
lp/cleanuprows = TRUE

# should new basic rows be removed after root LP solving?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
lp/cleanuprowsroot = TRUE

# should LP solver's return status be checked for stability?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
lp/checkstability = TRUE

# maximum condition number of LP basis counted as stable (-1.0: no limit)
# [type: real, range: [-1,1.79769313486232e+308], default: -1]
lp/conditionlimit = -1

# should LP solutions be checked for primal feasibility, resolving LP when numerical troubles occur?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
lp/checkprimfeas = TRUE

# should LP solutions be checked for dual feasibility, resolving LP when numerical troubles occur?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
lp/checkdualfeas = TRUE

# which FASTMIP setting of LP solver should be used? 0: off, 1: low
# [type: int, range: [0,1], default: 1]
lp/fastmip = 1

# should scaling of LP solver be used?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
lp/scaling = TRUE

# should presolving of LP solver be used?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
lp/presolving = TRUE

# should the lexicographic dual alogrithm be used?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
lp/lexdualalgo = FALSE

# should the lexicographic dual algorithm be applied only at the root node
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
lp/lexdualrootonly = TRUE

# maximum number of rounds in the  lexicographic dual algorithm (-1: unbounded)
# [type: int, range: [-1,2147483647], default: 2]
lp/lexdualmaxrounds = 2

# choose fractional basic variables in lexicographic dual algorithm?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
lp/lexdualbasic = FALSE

# turn on the lex dual algorithm only when stalling?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
lp/lexdualstalling = TRUE

# disable the cutoff bound in the LP solver? (0: enabled, 1: disabled, 2: auto)
# [type: int, range: [0,2], default: 2]
lp/disablecutoff = 2

# simplex algorithm shall use row representation of the basis if number of rows divided by number of columns exceeds this value (-1.0 to disable row representation)
# [type: real, range: [-1,1.79769313486232e+308], default: -1]
lp/rowrepswitch = -1

# number of threads used for solving the LP (0: automatic)
# [type: int, range: [0,64], default: 0]
lp/threads = 0

# factor of average LP iterations that is used as LP iteration limit for LP resolve (-1: unlimited)
# [type: real, range: [-1,1.79769313486232e+308], default: -1]
lp/resolveiterfac = -1

# minimum number of iterations that are allowed for LP resolve
# [type: int, range: [1,2147483647], default: 1000]
lp/resolveitermin = 1000

# solver to use for solving NLPs; leave empty to select NLPI with highest priority
# [type: string, default: ""]
nlp/solver = ""

# should the NLP relaxation be always disabled (also for NLPs/MINLPs)?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
nlp/disable = FALSE

# fraction of maximal memory usage resulting in switch to memory saving mode
# [type: real, range: [0,1], default: 0.8]
memory/savefac = 0.8

# memory growing factor for dynamically allocated arrays
# [type: real, range: [1,10], default: 1.2]
memory/arraygrowfac = 1.2

# initial size of dynamically allocated arrays
# [type: int, range: [0,2147483647], default: 4]
memory/arraygrowinit = 4

# memory growing factor for tree array
# [type: real, range: [1,10], default: 2]
memory/treegrowfac = 2

# initial size of tree array
# [type: int, range: [0,2147483647], default: 65536]
memory/treegrowinit = 65536

# memory growing factor for path array
# [type: real, range: [1,10], default: 2]
memory/pathgrowfac = 2

# initial size of path array
# [type: int, range: [0,2147483647], default: 256]
memory/pathgrowinit = 256

# should the CTRL-C interrupt be caught by SCIP?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
misc/catchctrlc = TRUE

# should a hashtable be used to map from variable names to variables?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
misc/usevartable = TRUE

# should a hashtable be used to map from constraint names to constraints?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
misc/useconstable = TRUE

# should smaller hashtables be used? yields better performance for small problems with about 100 variables
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
misc/usesmalltables = FALSE

# seed value for permuting the problem after the problem was transformed (-1: no permutation)
# [type: int, range: [-1,2147483647], default: -1]
misc/permutationseed = -1

# should order of constraints be permuted (depends on permutationseed)?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
misc/permuteconss = TRUE

# should order of variables be permuted (depends on permutationseed)?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
misc/permutevars = FALSE

# should the statistics be reset if the transformed problem is freed (in case of a Benders decomposition this parameter should be set to FALSE)
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
misc/resetstat = TRUE

# should only solutions be checked which improve the primal bound
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
misc/improvingsols = FALSE

# should the reason be printed if a given start solution is infeasible
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
misc/printreason = TRUE

# should the usage of external memory be estimated?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
misc/estimexternmem = TRUE

# should SCIP try to transfer original solutions to the extended space (after presolving)?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
misc/transorigsols = TRUE

# should SCIP calculate the primal dual integral value?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
misc/calcintegral = TRUE

# should SCIP try to remove infinite fixings from solutions copied to the solution store?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
misc/finitesolutionstore = FALSE

# child selection rule ('d'own, 'u'p, 'p'seudo costs, 'i'nference, 'l'p value, 'r'oot LP value difference, 'h'ybrid inference/root LP value difference)
# [type: char, range: {dupilrh}, default: h]
nodeselection/childsel = h

# values larger than this are considered infinity
# [type: real, range: [10000000000,1e+98], default: 1e+20]
numerics/infinity = 1e+20

# absolute values smaller than this are considered zero
# [type: real, range: [1e-20,0.001], default: 1e-09]
numerics/epsilon = 1e-09

# absolute values of sums smaller than this are considered zero
# [type: real, range: [1e-17,0.001], default: 1e-06]
numerics/sumepsilon = 1e-06

# feasibility tolerance for constraints
# [type: real, range: [1e-17,0.001], default: 1e-06]
numerics/feastol = 1e-06

# primal feasibility tolerance of LP solver
# [type: real, range: [1e-17,0.001], default: 1e-06]
numerics/lpfeastol = 1e-06

# feasibility tolerance for reduced costs in LP solution
# [type: real, range: [1e-17,0.001], default: 1e-07]
numerics/dualfeastol = 1e-07

# LP convergence tolerance used in barrier algorithm
# [type: real, range: [1e-17,0.001], default: 1e-10]
numerics/barrierconvtol = 1e-10

# minimal relative improve for strengthening bounds
# [type: real, range: [1e-17,1e+98], default: 0.05]
numerics/boundstreps = 0.05

# minimal variable distance value to use for branching pseudo cost updates
# [type: real, range: [1e-17,1], default: 0.1]
numerics/pseudocosteps = 0.1

# minimal objective distance value to use for branching pseudo cost updates
# [type: real, range: [0,1.79769313486232e+308], default: 0.0001]
numerics/pseudocostdelta = 0.0001

# minimal decrease factor that causes the recomputation of a value (e.g., pseudo objective) instead of an update
# [type: real, range: [0,1.79769313486232e+308], default: 10000000]
numerics/recomputefac = 10000000

# values larger than this are considered huge and should be handled separately (e.g., in activity computation)
# [type: real, range: [0,1e+98], default: 1e+15]
numerics/hugeval = 1e+15

# maximal number of presolving rounds (-1: unlimited, 0: off)
# [type: int, range: [-1,2147483647], default: -1]
presolving/maxrounds = -1

# abort presolve, if at most this fraction of the problem was changed in last presolve round
# [type: real, range: [0,1], default: 0.0001]
presolving/abortfac = 0.0001

# maximal number of restarts (-1: unlimited)
# [type: int, range: [-1,2147483647], default: -1]
presolving/maxrestarts = -1

# fraction of integer variables that were fixed in the root node triggering a restart with preprocessing after root node evaluation
# [type: real, range: [0,1], default: 0.05]
presolving/restartfac = 0.05

# fraction of integer variables that were fixed in the root node triggering an immediate restart with preprocessing
# [type: real, range: [0,1], default: 0.2]
presolving/immrestartfac = 0.2

# fraction of integer variables that were globally fixed during the solving process triggering a restart with preprocessing
# [type: real, range: [0,1], default: 1]
presolving/subrestartfac = 1

# minimal fraction of integer variables removed after restart to allow for an additional restart
# [type: real, range: [0,1], default: 0.1]
presolving/restartminred = 0.1

# should multi-aggregation of variables be forbidden?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
presolving/donotmultaggr = FALSE

# should aggregation of variables be forbidden?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
presolving/donotaggr = FALSE

# maximal number of variables priced in per pricing round
# [type: int, range: [1,2147483647], default: 100]
pricing/maxvars = 100

# maximal number of priced variables at the root node
# [type: int, range: [1,2147483647], default: 2000]
pricing/maxvarsroot = 2000

# pricing is aborted, if fac * pricing/maxvars pricing candidates were found
# [type: real, range: [1,1.79769313486232e+308], default: 2]
pricing/abortfac = 2

# should variables created at the current node be deleted when the node is solved in case they are not present in the LP anymore?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
pricing/delvars = FALSE

# should variables created at the root node be deleted when the root is solved in case they are not present in the LP anymore?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
pricing/delvarsroot = FALSE

# maximal number of propagation rounds per node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 100]
propagating/maxrounds = 100

# maximal number of propagation rounds in the root node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 1000]
propagating/maxroundsroot = 1000

# should propagation be aborted immediately? setting this to FALSE could help conflict analysis to produce more conflict constraints
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
propagating/abortoncutoff = TRUE

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying separation (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 1]
separating/maxbounddist = 1

# minimal efficacy for a cut to enter the LP
# [type: real, range: [0,1e+98], default: 0.05]
separating/minefficacy = 0.05

# minimal efficacy for a cut to enter the LP in the root node
# [type: real, range: [0,1e+98], default: 0.001]
separating/minefficacyroot = 0.001

# minimal orthogonality for a cut to enter the LP
# [type: real, range: [0,1], default: 0.5]
separating/minortho = 0.5

# minimal orthogonality for a cut to enter the LP in the root node
# [type: real, range: [0,1], default: 0.5]
separating/minorthoroot = 0.5

# factor to scale objective parallelism of cut in separation score calculation
# [type: real, range: [0,1e+98], default: 0.0001]
separating/objparalfac = 0.0001

# factor to scale orthogonality of cut in separation score calculation (0.0 to disable orthogonality calculation)
# [type: real, range: [0,1e+98], default: 1]
separating/orthofac = 1

# function used for calc. scalar prod. in orthogonality test ('e'uclidean, 'd'iscrete)
# [type: char, range: {ed}, default: e]
separating/orthofunc = e

# row norm to use for efficacy calculation ('e'uclidean, 'm'aximum, 's'um, 'd'iscrete)
# [type: char, range: {emsd}, default: e]
separating/efficacynorm = e

# maximal number of runs for which separation is enabled (-1: unlimited)
# [type: int, range: [-1,2147483647], default: -1]
separating/maxruns = -1

# maximal number of separation rounds per node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 5]
separating/maxrounds = 5

# maximal number of separation rounds in the root node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: -1]
separating/maxroundsroot = -1

# maximal number of separation rounds in the root node of a subsequent run (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 1]
separating/maxroundsrootsubrun = 1

# maximal additional number of separation rounds in subsequent price-and-cut loops (-1: no additional restriction)
# [type: int, range: [-1,2147483647], default: 1]
separating/maxaddrounds = 1

# maximal number of consecutive separation rounds without objective or integrality improvement (-1: no additional restriction)
# [type: int, range: [-1,2147483647], default: 5]
separating/maxstallrounds = 5

# maximal number of cuts separated per separation round (0: disable local separation)
# [type: int, range: [0,2147483647], default: 100]
separating/maxcuts = 100

# maximal number of separated cuts at the root node (0: disable root node separation)
# [type: int, range: [0,2147483647], default: 2000]
separating/maxcutsroot = 2000

# maximum age a cut can reach before it is deleted from the global cut pool, or -1 to keep all cuts
# [type: int, range: [-1,2147483647], default: 100]
separating/cutagelimit = 100

# separation frequency for the global cut pool (-1: disable global cut pool, 0: only separate pool at the root)
# [type: int, range: [-1,2147483647], default: 0]
separating/poolfreq = 0

# factor on cut infeasibility to limit feasibility tolerance for relaxation solver (-1: off)
# [type: real, range: [-1,1], default: -1]
separating/feastolfac = -1

# default clock type (1: CPU user seconds, 2: wall clock time)
# [type: int, range: [1,2], default: 1]
timing/clocktype = 1

# is timing enabled?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
timing/enabled = TRUE

# belongs reading time to solving time?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
timing/reading = FALSE

# should clock checks of solving time be performed less frequently (note: time limit could be exceeded slightly)
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
timing/rareclockcheck = FALSE

# should timing for statistic output be performed?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
timing/statistictiming = TRUE

# name of the VBC Tool output file, or - if no VBC Tool output should be created
# [type: string, default: "-"]
vbc/filename = "-"

# should the real solving time be used instead of a time step counter in VBC output?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
vbc/realtime = TRUE

# should the node where solutions are found be visualized?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
vbc/dispsols = FALSE

# should model constraints be marked as initial?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
reading/initialconss = TRUE

# should model constraints be subject to aging?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
reading/dynamicconss = TRUE

# should columns be added and removed dynamically to the LP?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
reading/dynamiccols = FALSE

# should rows be added and removed dynamically to the LP?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
reading/dynamicrows = FALSE

# should all constraints be written (including the redundant constraints)?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
write/allconss = FALSE

# when writing a generic problem the index for the first variable should start with?
# [type: int, range: [0,1073741823], default: 0]
write/genericnamesoffset = 0

# priority of conflict handler <linear>
# [type: int, range: [-2147483648,2147483647], default: -1000000]
conflict/linear/priority = -1000000

# frequency for separating cuts (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
constraints/linear/sepafreq = 0

# frequency for propagating domains (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
constraints/linear/propfreq = 1

# timing when constraint propagation should be called (1:BEFORELP, 2:DURINGLPLOOP, 4:AFTERLPLOOP, 15:ALWAYS)
# [type: int, range: [1,15], default: 1]
constraints/linear/timingmask = 1

# frequency for using all instead of only the useful constraints in separation, propagation and enforcement (-1: never, 0: only in first evaluation)
# [type: int, range: [-1,2147483647], default: 100]
constraints/linear/eagerfreq = 100

# maximal number of presolving rounds the constraint handler participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
constraints/linear/maxprerounds = -1

# should separation method be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/linear/delaysepa = FALSE

# should propagation method be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/linear/delayprop = FALSE

# should presolving method be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/linear/delaypresol = FALSE

# multiplier on propagation frequency, how often the bounds are tightened (-1: never, 0: only at root)
# [type: int, range: [-1,2147483647], default: 1]
constraints/linear/tightenboundsfreq = 1

# maximal number of separation rounds per node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 5]
constraints/linear/maxrounds = 5

# maximal number of separation rounds per node in the root node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: -1]
constraints/linear/maxroundsroot = -1

# maximal number of cuts separated per separation round
# [type: int, range: [0,2147483647], default: 50]
constraints/linear/maxsepacuts = 50

# maximal number of cuts separated per separation round in the root node
# [type: int, range: [0,2147483647], default: 200]
constraints/linear/maxsepacutsroot = 200

# should pairwise constraint comparison be performed in presolving?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/linear/presolpairwise = TRUE

# should hash table be used for detecting redundant constraints in advance
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/linear/presolusehashing = TRUE

# number for minimal pairwise presolve comparisons
# [type: int, range: [1,2147483647], default: 200000]
constraints/linear/nmincomparisons = 200000

# minimal gain per minimal pairwise presolve comparisons to repeat pairwise comparison round
# [type: real, range: [0,1], default: 1e-06]
constraints/linear/mingainpernmincomparisons = 1e-06

# maximal allowed relative gain in maximum norm for constraint aggregation (0.0: disable constraint aggregation)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
constraints/linear/maxaggrnormscale = 0

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for separating knapsack cardinality cuts
# [type: real, range: [0,1], default: 0]
constraints/linear/maxcardbounddist = 0

# should all constraints be subject to cardinality cut generation instead of only the ones with non-zero dual value?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/linear/separateall = FALSE

# should presolving search for aggregations in equations
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/linear/aggregatevariables = TRUE

# should presolving try to simplify inequalities
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/linear/simplifyinequalities = TRUE

# should dual presolving steps be performed?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/linear/dualpresolving = TRUE

# apply binaries sorting in decr. order of coeff abs value?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/linear/sortvars = TRUE

# should the violation for a constraint with side 0.0 be checked relative to 1.0 (FALSE) or to the maximum absolute value in the activity (TRUE)?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/linear/checkrelmaxabs = FALSE

# should presolving try to detect constraints parallel to the objective function defining an upper bound and prevent these constraints from entering the LP?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/linear/detectcutoffbound = TRUE

# should presolving try to detect constraints parallel to the objective function defining a lower bound and prevent these constraints from entering the LP?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/linear/detectlowerbound = TRUE

# should presolving try to detect subsets of constraints parallel to the objective function?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/linear/detectpartialobjective = TRUE

# frequency for separating cuts (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: -1]
constraints/integral/sepafreq = -1

# frequency for propagating domains (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: -1]
constraints/integral/propfreq = -1

# timing when constraint propagation should be called (1:BEFORELP, 2:DURINGLPLOOP, 4:AFTERLPLOOP, 15:ALWAYS)
# [type: int, range: [1,15], default: 1]
constraints/integral/timingmask = 1

# frequency for using all instead of only the useful constraints in separation, propagation and enforcement (-1: never, 0: only in first evaluation)
# [type: int, range: [-1,2147483647], default: -1]
constraints/integral/eagerfreq = -1

# maximal number of presolving rounds the constraint handler participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: 0]
constraints/integral/maxprerounds = 0

# should separation method be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/integral/delaysepa = FALSE

# should propagation method be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/integral/delayprop = FALSE

# should presolving method be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/integral/delaypresol = FALSE

# frequency for separating cuts (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
constraints/knapsack/sepafreq = 0

# frequency for propagating domains (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
constraints/knapsack/propfreq = 1

# timing when constraint propagation should be called (1:BEFORELP, 2:DURINGLPLOOP, 4:AFTERLPLOOP, 15:ALWAYS)
# [type: int, range: [1,15], default: 1]
constraints/knapsack/timingmask = 1

# frequency for using all instead of only the useful constraints in separation, propagation and enforcement (-1: never, 0: only in first evaluation)
# [type: int, range: [-1,2147483647], default: 100]
constraints/knapsack/eagerfreq = 100

# maximal number of presolving rounds the constraint handler participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
constraints/knapsack/maxprerounds = -1

# should separation method be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/knapsack/delaysepa = FALSE

# should propagation method be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/knapsack/delayprop = FALSE

# should presolving method be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/knapsack/delaypresol = FALSE

# enable linear upgrading for constraint handler <knapsack>
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/linear/upgrade/knapsack = TRUE

# multiplier on separation frequency, how often knapsack cuts are separated (-1: never, 0: only at root)
# [type: int, range: [-1,2147483647], default: 1]
constraints/knapsack/sepacardfreq = 1

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for separating knapsack cuts
# [type: real, range: [0,1], default: 0]
constraints/knapsack/maxcardbounddist = 0

# maximal number of separation rounds per node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 5]
constraints/knapsack/maxrounds = 5

# maximal number of separation rounds per node in the root node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: -1]
constraints/knapsack/maxroundsroot = -1

# maximal number of cuts separated per separation round
# [type: int, range: [0,2147483647], default: 50]
constraints/knapsack/maxsepacuts = 50

# maximal number of cuts separated per separation round in the root node
# [type: int, range: [0,2147483647], default: 200]
constraints/knapsack/maxsepacutsroot = 200

# should disaggregation of knapsack constraints be allowed in preprocessing?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/knapsack/disaggregation = TRUE

# should presolving try to simplify knapsacks
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/knapsack/simplifyinequalities = TRUE

# should negated clique information be used in solving process
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/knapsack/negatedclique = TRUE

# should pairwise constraint comparison be performed in presolving?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/knapsack/presolpairwise = TRUE

# should hash table be used for detecting redundant constraints in advance
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/knapsack/presolusehashing = TRUE

# should dual presolving steps be performed?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/knapsack/dualpresolving = TRUE

# should GUB information be used for separation?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/knapsack/usegubs = FALSE

# should presolving try to detect constraints parallel to the objective function defining an upper bound and prevent these constraints from entering the LP?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/knapsack/detectcutoffbound = TRUE

# should presolving try to detect constraints parallel to the objective function defining a lower bound and prevent these constraints from entering the LP?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/knapsack/detectlowerbound = TRUE

# priority of conflict handler <logicor>
# [type: int, range: [-2147483648,2147483647], default: 800000]
conflict/logicor/priority = 800000

# frequency for separating cuts (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
constraints/logicor/sepafreq = 0

# frequency for propagating domains (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
constraints/logicor/propfreq = 1

# timing when constraint propagation should be called (1:BEFORELP, 2:DURINGLPLOOP, 4:AFTERLPLOOP, 15:ALWAYS)
# [type: int, range: [1,15], default: 1]
constraints/logicor/timingmask = 1

# frequency for using all instead of only the useful constraints in separation, propagation and enforcement (-1: never, 0: only in first evaluation)
# [type: int, range: [-1,2147483647], default: 100]
constraints/logicor/eagerfreq = 100

# maximal number of presolving rounds the constraint handler participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
constraints/logicor/maxprerounds = -1

# should separation method be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/logicor/delaysepa = FALSE

# should propagation method be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/logicor/delayprop = FALSE

# should presolving method be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/logicor/delaypresol = FALSE

# enable linear upgrading for constraint handler <logicor>
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/linear/upgrade/logicor = TRUE

# should pairwise constraint comparison be performed in presolving?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/logicor/presolpairwise = TRUE

# should hash table be used for detecting redundant constraints in advance
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/logicor/presolusehashing = TRUE

# should dual presolving steps be performed?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/logicor/dualpresolving = TRUE

# should negated clique information be used in presolving
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/logicor/negatedclique = TRUE

# should implications/cliques be used in presolving
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/logicor/implications = TRUE

# should pairwise constraint comparison try to strengthen constraints by removing superflous non-zeros?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/logicor/strengthen = TRUE

# priority of conflict handler <setppc>
# [type: int, range: [-2147483648,2147483647], default: 700000]
conflict/setppc/priority = 700000

# frequency for separating cuts (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
constraints/setppc/sepafreq = 0

# frequency for propagating domains (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
constraints/setppc/propfreq = 1

# timing when constraint propagation should be called (1:BEFORELP, 2:DURINGLPLOOP, 4:AFTERLPLOOP, 15:ALWAYS)
# [type: int, range: [1,15], default: 1]
constraints/setppc/timingmask = 1

# frequency for using all instead of only the useful constraints in separation, propagation and enforcement (-1: never, 0: only in first evaluation)
# [type: int, range: [-1,2147483647], default: 100]
constraints/setppc/eagerfreq = 100

# maximal number of presolving rounds the constraint handler participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
constraints/setppc/maxprerounds = -1

# should separation method be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/setppc/delaysepa = FALSE

# should propagation method be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/setppc/delayprop = FALSE

# should presolving method be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/setppc/delaypresol = FALSE

# enable linear upgrading for constraint handler <setppc>
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/linear/upgrade/setppc = TRUE

# number of children created in pseudo branching (0: disable pseudo branching)
# [type: int, range: [0,2147483647], default: 2]
constraints/setppc/npseudobranches = 2

# should pairwise constraint comparison be performed in presolving?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/setppc/presolpairwise = TRUE

# should hash table be used for detecting redundant constraints in advance
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/setppc/presolusehashing = TRUE

# should dual presolving steps be performed?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/setppc/dualpresolving = TRUE

#  should we try to lift variables into other clique constraints, fix variables, aggregate them, and also shrink the amount of variables in clique constraints
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/setppc/cliquelifting = FALSE

# should we try to generate extra cliques out of all binary variables to maybe fasten redundant constraint detection
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/setppc/addvariablesascliques = FALSE

# should we try to shrink the number of variables in a clique constraints, by replacing more than one variable by only one
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/setppc/cliqueshrinking = TRUE

# frequency for separating cuts (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
constraints/varbound/sepafreq = 0

# frequency for propagating domains (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
constraints/varbound/propfreq = 1

# timing when constraint propagation should be called (1:BEFORELP, 2:DURINGLPLOOP, 4:AFTERLPLOOP, 15:ALWAYS)
# [type: int, range: [1,15], default: 1]
constraints/varbound/timingmask = 1

# frequency for using all instead of only the useful constraints in separation, propagation and enforcement (-1: never, 0: only in first evaluation)
# [type: int, range: [-1,2147483647], default: 100]
constraints/varbound/eagerfreq = 100

# maximal number of presolving rounds the constraint handler participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
constraints/varbound/maxprerounds = -1

# should separation method be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/varbound/delaysepa = FALSE

# should propagation method be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/varbound/delayprop = FALSE

# should presolving method be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/varbound/delaypresol = FALSE

# enable linear upgrading for constraint handler <varbound>
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/linear/upgrade/varbound = TRUE

# should pairwise constraint comparison be performed in presolving?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/varbound/presolpairwise = TRUE

# maximum coefficient in varbound constraint to be added as a row into LP
# [type: real, range: [0,1e+20], default: 1000000000]
constraints/varbound/maxlpcoef = 1000000000

# should bound widening be used in conflict analysis?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/varbound/usebdwidening = TRUE

# should fixed and aggregated variables be printed (if not, re-parsing might fail)
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
reading/cipreader/writefixedvars = TRUE

# should an artificial objective, depending on the number of clauses a variable appears in, be used?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
reading/cnfreader/useobj = FALSE

# have integer variables no upper bound by default (depending on GAMS version)?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
reading/gmsreader/freeints = FALSE

# shall characters '#', '*', '+', '/', and '-' in variable and constraint names be replaced by '_'?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
reading/gmsreader/replaceforbiddenchars = FALSE

# default M value for big-M reformulation of indicator constraints in case no bound on slack variable is given
# [type: real, range: [0,1.79769313486232e+308], default: 1000000]
reading/gmsreader/bigmdefault = 1000000

# which reformulation to use for indicator constraints: 'b'ig-M, 's'os1
# [type: char, range: {bs}, default: s]
reading/gmsreader/indicatorreform = s

# is it allowed to use the gams function signpower(x,a)?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
reading/gmsreader/signpower = FALSE

# should possible "and" constraint be linearized when writing the lp file?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
reading/lpreader/linearize-and-constraints = TRUE

# should an aggregated linearization for and constraints be used?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
reading/lpreader/aggrlinearization-ands = TRUE

# should possible "and" constraint be linearized when writing the mps file?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
reading/mpsreader/linearize-and-constraints = TRUE

# should an aggregated linearization for and constraints be used?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
reading/mpsreader/aggrlinearization-ands = TRUE

# should model constraints be subject to aging?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
reading/opbreader/dynamicconss = FALSE

# use '*' between coefficients and variables by writing to problem?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
reading/opbreader/multisymbol = FALSE

# should the output format be binary(P4) (otherwise plain(P1) format)
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
reading/pbmreader/binary = TRUE

# maximum number of rows in the scaled picture (-1 for no limit)
# [type: int, range: [-1,2147483647], default: 1000]
reading/pbmreader/maxrows = 1000

# maximum number of columns in the scaled picture (-1 for no limit)
# [type: int, range: [-1,2147483647], default: 1000]
reading/pbmreader/maxcols = 1000

# priority of presolver <boundshift>
# [type: int, range: [-536870912,536870911], default: 7900000]
presolving/boundshift/priority = 7900000

# maximal number of presolving rounds the presolver participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: 0]
presolving/boundshift/maxrounds = 0

# should presolver be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
presolving/boundshift/delay = FALSE

# absolute value of maximum shift
# [type: longint, range: [0,9223372036854775807], default: 9223372036854775807]
presolving/boundshift/maxshift = 9223372036854775807

# is flipping allowed (multiplying with -1)?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
presolving/boundshift/flipping = TRUE

# shift only integer ranges?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
presolving/boundshift/integer = TRUE

# priority of presolver <implics>
# [type: int, range: [-536870912,536870911], default: -10000]
presolving/implics/priority = -10000

# maximal number of presolving rounds the presolver participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
presolving/implics/maxrounds = -1

# should presolver be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
presolving/implics/delay = FALSE

# priority of presolver <inttobinary>
# [type: int, range: [-536870912,536870911], default: 7000000]
presolving/inttobinary/priority = 7000000

# maximal number of presolving rounds the presolver participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
presolving/inttobinary/maxrounds = -1

# should presolver be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
presolving/inttobinary/delay = FALSE

# priority of presolver <trivial>
# [type: int, range: [-536870912,536870911], default: 9000000]
presolving/trivial/priority = 9000000

# maximal number of presolving rounds the presolver participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
presolving/trivial/maxrounds = -1

# should presolver be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
presolving/trivial/delay = FALSE

# priority of presolver <components>
# [type: int, range: [-536870912,536870911], default: -9200000]
presolving/components/priority = -9200000

# maximal number of presolving rounds the presolver participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
presolving/components/maxrounds = -1

# should presolver be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
presolving/components/delay = TRUE

# should the single components be written as an .lp-file?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
presolving/components/writeproblems = FALSE

# maximum number of integer (or binary) variables to solve a subproblem directly (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 500]
presolving/components/maxintvars = 500

# maximum number of nodes to be solved in subproblems
# [type: longint, range: [-1,9223372036854775807], default: 10000]
presolving/components/nodelimit = 10000

# the weight of an integer variable compared to binary variables
# [type: real, range: [0,1.79769313486232e+308], default: 1]
presolving/components/intfactor = 1

# percentage by which the number of variables has to be decreased after the last component solving to allow running again (1.0: do not run again)
# [type: real, range: [0,1], default: 0.2]
presolving/components/reldecrease = 0.2

# factor to increase the feasibility tolerance of the main SCIP in all sub-SCIPs, default value 1.0
# [type: real, range: [0,1000000], default: 1]
presolving/components/feastolfactor = 1

# priority of presolver <domcol>
# [type: int, range: [-536870912,536870911], default: 20000000]
presolving/domcol/priority = 20000000

# maximal number of presolving rounds the presolver participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
presolving/domcol/maxrounds = -1

# should presolver be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
presolving/domcol/delay = TRUE

# minimal number of pair comparisons
# [type: int, range: [100,1048576], default: 1024]
presolving/domcol/numminpairs = 1024

# maximal number of pair comparisons
# [type: int, range: [1024,1000000000], default: 1048576]
presolving/domcol/nummaxpairs = 1048576

# should predictive bound strengthening be applied?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
presolving/domcol/predbndstr = FALSE

# should singleton columns stuffing be applied?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
presolving/domcol/singcolstuffing = TRUE

# priority of presolver <convertinttobin>
# [type: int, range: [-536870912,536870911], default: 6000000]
presolving/convertinttobin/priority = 6000000

# maximal number of presolving rounds the presolver participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: 0]
presolving/convertinttobin/maxrounds = 0

# should presolver be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
presolving/convertinttobin/delay = FALSE

# absolute value of maximum domain size for converting an integer variable to binaries variables
# [type: longint, range: [0,9223372036854775807], default: 9223372036854775807]
presolving/convertinttobin/maxdomainsize = 9223372036854775807

# should only integer variables with a domain size of 2^p - 1 be converted(, there we don't need an knapsack-constraint for restricting the sum of the binaries)
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
presolving/convertinttobin/onlypoweroftwo = FALSE

# should only integer variables with uplocks equals downlocks be converted
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
presolving/convertinttobin/samelocksinbothdirections = FALSE

# priority of node selection rule <bfs> in standard mode
# [type: int, range: [-536870912,536870911], default: 100000]
nodeselection/bfs/stdpriority = 100000

# priority of node selection rule <bfs> in memory saving mode
# [type: int, range: [-536870912,536870911], default: 0]
nodeselection/bfs/memsavepriority = 0

# minimal plunging depth, before new best node may be selected (-1 for dynamic setting)
# [type: int, range: [-1,2147483647], default: -1]
nodeselection/bfs/minplungedepth = -1

# maximal plunging depth, before new best node is forced to be selected (-1 for dynamic setting)
# [type: int, range: [-1,2147483647], default: -1]
nodeselection/bfs/maxplungedepth = -1

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where plunging is performed
# [type: real, range: [0,1.79769313486232e+308], default: 0.25]
nodeselection/bfs/maxplungequot = 0.25

# priority of node selection rule <dfs> in standard mode
# [type: int, range: [-536870912,536870911], default: 0]
nodeselection/dfs/stdpriority = 0

# priority of node selection rule <dfs> in memory saving mode
# [type: int, range: [-536870912,536870911], default: 100000]
nodeselection/dfs/memsavepriority = 100000

# priority of node selection rule <estimate> in standard mode
# [type: int, range: [-536870912,536870911], default: 200000]
nodeselection/estimate/stdpriority = 200000

# priority of node selection rule <estimate> in memory saving mode
# [type: int, range: [-536870912,536870911], default: 100]
nodeselection/estimate/memsavepriority = 100

# minimal plunging depth, before new best node may be selected (-1 for dynamic setting)
# [type: int, range: [-1,2147483647], default: -1]
nodeselection/estimate/minplungedepth = -1

# maximal plunging depth, before new best node is forced to be selected (-1 for dynamic setting)
# [type: int, range: [-1,2147483647], default: -1]
nodeselection/estimate/maxplungedepth = -1

# maximal quotient (estimate - lowerbound)/(cutoffbound - lowerbound) where plunging is performed
# [type: real, range: [0,1.79769313486232e+308], default: 0.25]
nodeselection/estimate/maxplungequot = 0.25

# frequency at which the best node instead of the best estimate is selected (0: never)
# [type: int, range: [0,2147483647], default: 10]
nodeselection/estimate/bestnodefreq = 10

# depth until breadth-first search is applied
# [type: int, range: [-1,2147483647], default: -1]
nodeselection/estimate/breadthfirstdepth = -1

# number of nodes before doing plunging the first time
# [type: int, range: [0,2147483647], default: 0]
nodeselection/estimate/plungeoffset = 0

# priority of node selection rule <hybridestim> in standard mode
# [type: int, range: [-536870912,536870911], default: 50000]
nodeselection/hybridestim/stdpriority = 50000

# priority of node selection rule <hybridestim> in memory saving mode
# [type: int, range: [-536870912,536870911], default: 50]
nodeselection/hybridestim/memsavepriority = 50

# minimal plunging depth, before new best node may be selected (-1 for dynamic setting)
# [type: int, range: [-1,2147483647], default: -1]
nodeselection/hybridestim/minplungedepth = -1

# maximal plunging depth, before new best node is forced to be selected (-1 for dynamic setting)
# [type: int, range: [-1,2147483647], default: -1]
nodeselection/hybridestim/maxplungedepth = -1

# maximal quotient (estimate - lowerbound)/(cutoffbound - lowerbound) where plunging is performed
# [type: real, range: [0,1.79769313486232e+308], default: 0.25]
nodeselection/hybridestim/maxplungequot = 0.25

# frequency at which the best node instead of the hybrid best estimate / best bound is selected (0: never)
# [type: int, range: [0,2147483647], default: 1000]
nodeselection/hybridestim/bestnodefreq = 1000

# weight of estimate value in node selection score (0: pure best bound search, 1: pure best estimate search)
# [type: real, range: [0,1], default: 0.1]
nodeselection/hybridestim/estimweight = 0.1

# priority of node selection rule <restartdfs> in standard mode
# [type: int, range: [-536870912,536870911], default: 10000]
nodeselection/restartdfs/stdpriority = 10000

# priority of node selection rule <restartdfs> in memory saving mode
# [type: int, range: [-536870912,536870911], default: 50000]
nodeselection/restartdfs/memsavepriority = 50000

# frequency for selecting the best node instead of the deepest one
# [type: int, range: [0,2147483647], default: 100]
nodeselection/restartdfs/selectbestfreq = 100

# count only leaf nodes (otherwise all nodes)?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
nodeselection/restartdfs/countonlyleaves = TRUE

# priority of propagator <dualfix>
# [type: int, range: [-536870912,536870911], default: 8000000]
propagating/dualfix/priority = 8000000

# frequency for calling propagator <dualfix> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
propagating/dualfix/freq = 0

# should propagator be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
propagating/dualfix/delay = FALSE

# timing when propagator should be called (1:BEFORELP, 2:DURINGLPLOOP, 4:AFTERLPLOOP, 15:ALWAYS))
# [type: int, range: [1,15], default: 1]
propagating/dualfix/timingmask = 1

# presolving priority of propagator <dualfix>
# [type: int, range: [-536870912,536870911], default: 8000000]
propagating/dualfix/presolpriority = 8000000

# maximal number of presolving rounds the propagator participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
propagating/dualfix/maxprerounds = -1

# should presolving be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
propagating/dualfix/presoldelay = FALSE

# priority of propagator <pseudoobj>
# [type: int, range: [-536870912,536870911], default: 3000000]
propagating/pseudoobj/priority = 3000000

# frequency for calling propagator <pseudoobj> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
propagating/pseudoobj/freq = 1

# should propagator be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
propagating/pseudoobj/delay = FALSE

# timing when propagator should be called (1:BEFORELP, 2:DURINGLPLOOP, 4:AFTERLPLOOP, 15:ALWAYS))
# [type: int, range: [1,15], default: 7]
propagating/pseudoobj/timingmask = 7

# presolving priority of propagator <pseudoobj>
# [type: int, range: [-536870912,536870911], default: 6000000]
propagating/pseudoobj/presolpriority = 6000000

# maximal number of presolving rounds the propagator participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
propagating/pseudoobj/maxprerounds = -1

# should presolving be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
propagating/pseudoobj/presoldelay = FALSE

# minimal number of successive none binary variable propagator whithout a bound reduction before aborted
# [type: int, range: [0,2147483647], default: 100]
propagating/pseudoobj/minuseless = 100

# maximal fraction of none binary variables with non-zero objective without a bound reduction before aborted
# [type: real, range: [0,1], default: 0.1]
propagating/pseudoobj/maxvarsfrac = 0.1

# do we want to propagate all none binary variables if we are propagating the root node
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
propagating/pseudoobj/propfullinroot = TRUE

# propagate new cutoff bound directly globally
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
propagating/pseudoobj/propcutoffbound = TRUE

# should the propagator be forced even active pricer are present?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
propagating/pseudoobj/force = FALSE

# number of variable added after the propgatore is reinitialized?
# [type: int, range: [0,2147483647], default: 1000]
propagating/pseudoobj/maxnewvars = 1000

# use implications to strengthen the propagation of binary variable (increasing the objective change)?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
propagating/pseudoobj/propuseimplics = TRUE

# use implications to strengthen the resolve propagation of binary variable (increasing the objective change)?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
propagating/pseudoobj/respropuseimplics = TRUE

# maximum number of binary variables the implications are used if turned on (-1: unlimited)?
# [type: int, range: [-1,2147483647], default: 50000]
propagating/pseudoobj/maximplvars = 50000

# priority of propagator <rootredcost>
# [type: int, range: [-536870912,536870911], default: 10000000]
propagating/rootredcost/priority = 10000000

# frequency for calling propagator <rootredcost> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
propagating/rootredcost/freq = 1

# should propagator be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
propagating/rootredcost/delay = FALSE

# timing when propagator should be called (1:BEFORELP, 2:DURINGLPLOOP, 4:AFTERLPLOOP, 15:ALWAYS))
# [type: int, range: [1,15], default: 5]
propagating/rootredcost/timingmask = 5

# presolving priority of propagator <rootredcost>
# [type: int, range: [-536870912,536870911], default: 0]
propagating/rootredcost/presolpriority = 0

# maximal number of presolving rounds the propagator participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
propagating/rootredcost/maxprerounds = -1

# should presolving be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
propagating/rootredcost/presoldelay = FALSE

# priority of propagator <genvbounds>
# [type: int, range: [-536870912,536870911], default: 3000000]
propagating/genvbounds/priority = 3000000

# frequency for calling propagator <genvbounds> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
propagating/genvbounds/freq = 1

# should propagator be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
propagating/genvbounds/delay = FALSE

# timing when propagator should be called (1:BEFORELP, 2:DURINGLPLOOP, 4:AFTERLPLOOP, 15:ALWAYS))
# [type: int, range: [1,15], default: 15]
propagating/genvbounds/timingmask = 15

# presolving priority of propagator <genvbounds>
# [type: int, range: [-536870912,536870911], default: -2000000]
propagating/genvbounds/presolpriority = -2000000

# maximal number of presolving rounds the propagator participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
propagating/genvbounds/maxprerounds = -1

# should presolving be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
propagating/genvbounds/presoldelay = FALSE

# apply global propagation?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
propagating/genvbounds/global = TRUE

# apply genvbounds in root node if no new incumbent was found?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
propagating/genvbounds/propinrootnode = TRUE

# sort genvbounds and wait for bound change events?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
propagating/genvbounds/sort = TRUE

# priority of propagator <probing>
# [type: int, range: [-536870912,536870911], default: -100000]
propagating/probing/priority = -100000

# frequency for calling propagator <probing> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: -1]
propagating/probing/freq = -1

# should propagator be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
propagating/probing/delay = TRUE

# timing when propagator should be called (1:BEFORELP, 2:DURINGLPLOOP, 4:AFTERLPLOOP, 15:ALWAYS))
# [type: int, range: [1,15], default: 4]
propagating/probing/timingmask = 4

# presolving priority of propagator <probing>
# [type: int, range: [-536870912,536870911], default: -100000]
propagating/probing/presolpriority = -100000

# maximal number of presolving rounds the propagator participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
propagating/probing/maxprerounds = -1

# should presolving be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
propagating/probing/presoldelay = TRUE

# maximal number of runs, probing participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: 1]
propagating/probing/maxruns = 1

# maximal number of propagation rounds in probing subproblems (-1: no limit, 0: auto)
# [type: int, range: [-1,2147483647], default: -1]
propagating/probing/proprounds = -1

# maximal number of fixings found, until probing is interrupted (0: don't iterrupt)
# [type: int, range: [0,2147483647], default: 25]
propagating/probing/maxfixings = 25

# maximal number of successive probings without fixings, until probing is aborted (0: don't abort)
# [type: int, range: [0,2147483647], default: 1000]
propagating/probing/maxuseless = 1000

# maximal number of successive probings without fixings, bound changes, and implications, until probing is aborted (0: don't abort)
# [type: int, range: [0,2147483647], default: 50]
propagating/probing/maxtotaluseless = 50

# maximal number of probings without fixings, until probing is aborted (0: don't abort)
# [type: int, range: [0,2147483647], default: 0]
propagating/probing/maxsumuseless = 0

# maximal depth until propagation is executed(-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
propagating/probing/maxdepth = -1

# priority of propagator <redcost>
# [type: int, range: [-536870912,536870911], default: 1000000]
propagating/redcost/priority = 1000000

# frequency for calling propagator <redcost> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
propagating/redcost/freq = 1

# should propagator be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
propagating/redcost/delay = FALSE

# timing when propagator should be called (1:BEFORELP, 2:DURINGLPLOOP, 4:AFTERLPLOOP, 15:ALWAYS))
# [type: int, range: [1,15], default: 6]
propagating/redcost/timingmask = 6

# presolving priority of propagator <redcost>
# [type: int, range: [-536870912,536870911], default: 0]
propagating/redcost/presolpriority = 0

# maximal number of presolving rounds the propagator participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
propagating/redcost/maxprerounds = -1

# should presolving be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
propagating/redcost/presoldelay = FALSE

# should reduced cost fixing be also applied to continuous variables?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
propagating/redcost/continuous = FALSE

# should implications be used to strength the reduced cost for binary variables?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
propagating/redcost/useimplics = TRUE

# priority of propagator <vbounds>
# [type: int, range: [-536870912,536870911], default: 3000000]
propagating/vbounds/priority = 3000000

# frequency for calling propagator <vbounds> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
propagating/vbounds/freq = 1

# should propagator be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
propagating/vbounds/delay = FALSE

# timing when propagator should be called (1:BEFORELP, 2:DURINGLPLOOP, 4:AFTERLPLOOP, 15:ALWAYS))
# [type: int, range: [1,15], default: 5]
propagating/vbounds/timingmask = 5

# presolving priority of propagator <vbounds>
# [type: int, range: [-536870912,536870911], default: 0]
propagating/vbounds/presolpriority = 0

# maximal number of presolving rounds the propagator participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
propagating/vbounds/maxprerounds = -1

# should presolving be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
propagating/vbounds/presoldelay = FALSE

# should bound widening be used to initialize conflict analysis?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
propagating/vbounds/usebdwidening = TRUE

# should implications be propagated?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
propagating/vbounds/useimplics = FALSE

# should cliques be propagated?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
propagating/vbounds/usecliques = FALSE

# should vbounds be propagated?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
propagating/vbounds/usevbounds = TRUE

# should the bounds be topologically sorted in advance?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
propagating/vbounds/dotoposort = TRUE

# should cliques be regarded for the topological sort?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
propagating/vbounds/sortcliques = FALSE

# priority of propagator <obbt>
# [type: int, range: [-536870912,536870911], default: -1000000]
propagating/obbt/priority = -1000000

# frequency for calling propagator <obbt> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
propagating/obbt/freq = 0

# should propagator be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
propagating/obbt/delay = TRUE

# timing when propagator should be called (1:BEFORELP, 2:DURINGLPLOOP, 4:AFTERLPLOOP, 15:ALWAYS))
# [type: int, range: [1,15], default: 4]
propagating/obbt/timingmask = 4

# presolving priority of propagator <obbt>
# [type: int, range: [-536870912,536870911], default: 0]
propagating/obbt/presolpriority = 0

# maximal number of presolving rounds the propagator participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
propagating/obbt/maxprerounds = -1

# should presolving be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
propagating/obbt/presoldelay = FALSE

# should obbt try to provide genvbounds if possible?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
propagating/obbt/creategenvbounds = TRUE

# should coefficients in filtering be normalized w.r.t. the domains sizes?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
propagating/obbt/normalize = TRUE

# try to filter bounds in so-called filter rounds by solving auxiliary LPs?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
propagating/obbt/applyfilterrounds = FALSE

# minimal number of filtered bounds to apply another filter round
# [type: int, range: [1,2147483647], default: 2]
propagating/obbt/minfilter = 2

# maximal number of bounds evaluated without success per group (-1: no limit)
# [type: int, range: [-1,2147483647], default: 3]
propagating/obbt/maxlookahead = 3

# multiple of root node LP iterations used as total LP iteration limit for obbt (<= 0: no limit )
# [type: real, range: [-1.79769313486232e+308,1.79769313486232e+308], default: 5]
propagating/obbt/itlimitfactor = 5

# feasibility tolerance for reduced costs used in obbt; this value is used if SCIP's dual feastol is greater
# [type: real, range: [0,1.79769313486232e+308], default: 1e-09]
propagating/obbt/dualfeastol = 1e-09

# maximum condition limit used in LP solver (-1.0: no limit)
# [type: real, range: [-1,1.79769313486232e+308], default: -1]
propagating/obbt/conditionlimit = -1

# priority of heuristic <actconsdiving>
# [type: int, range: [-536870912,536870911], default: -1003700]
heuristics/actconsdiving/priority = -1003700

# frequency for calling primal heuristic <actconsdiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/actconsdiving/freq = -1

# frequency offset for calling primal heuristic <actconsdiving>
# [type: int, range: [0,2147483647], default: 5]
heuristics/actconsdiving/freqofs = 5

# maximal depth level to call primal heuristic <actconsdiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/actconsdiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/actconsdiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/actconsdiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.05]
heuristics/actconsdiving/maxlpiterquot = 0.05

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/actconsdiving/maxlpiterofs = 1000

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/actconsdiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/actconsdiving/maxdiveavgquot = 0

# maximal UBQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1], default: 0.1]
heuristics/actconsdiving/maxdiveubquotnosol = 0.1

# maximal AVGQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/actconsdiving/maxdiveavgquotnosol = 0

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/actconsdiving/backtrack = TRUE

# priority of heuristic <clique>
# [type: int, range: [-536870912,536870911], default: -1000500]
heuristics/clique/priority = -1000500

# frequency for calling primal heuristic <clique> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/clique/freq = -1

# frequency offset for calling primal heuristic <clique>
# [type: int, range: [0,2147483647], default: 0]
heuristics/clique/freqofs = 0

# maximal depth level to call primal heuristic <clique> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/clique/maxdepth = -1

# value to increase nodenumber to determine the next run
# [type: real, range: [0,1.79769313486232e+308], default: 1.1]
heuristics/clique/multiplier = 1.1

# initial random seed value to permutate variables
# [type: int, range: [0,2147483647], default: 0]
heuristics/clique/initseed = 0

# minimum percentage of integer variables that have to be fixable
# [type: real, range: [0,1], default: 0.5]
heuristics/clique/minfixingrate = 0.5

# maximum number of nodes to regard in the subproblem
# [type: longint, range: [0,9223372036854775807], default: 5000]
heuristics/clique/maxnodes = 5000

# number of nodes added to the contingent of the total nodes
# [type: longint, range: [0,9223372036854775807], default: 500]
heuristics/clique/nodesofs = 500

# minimum number of nodes required to start the subproblem
# [type: longint, range: [0,9223372036854775807], default: 500]
heuristics/clique/minnodes = 500

# contingent of sub problem nodes in relation to the number of nodes of the original problem
# [type: real, range: [0,1], default: 0.1]
heuristics/clique/nodesquot = 0.1

# factor by which clique heuristic should at least improve the incumbent
# [type: real, range: [0,1], default: 0.01]
heuristics/clique/minimprove = 0.01

# maximum number of propagation rounds during probing (-1 infinity)
# [type: int, range: [-1,536870911], default: 2]
heuristics/clique/maxproprounds = 2

# should all active cuts from cutpool be copied to constraints in subproblem?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/clique/copycuts = TRUE

# priority of heuristic <coefdiving>
# [type: int, range: [-536870912,536870911], default: -1001000]
heuristics/coefdiving/priority = -1001000

# frequency for calling primal heuristic <coefdiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/coefdiving/freq = 10

# frequency offset for calling primal heuristic <coefdiving>
# [type: int, range: [0,2147483647], default: 1]
heuristics/coefdiving/freqofs = 1

# maximal depth level to call primal heuristic <coefdiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/coefdiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/coefdiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/coefdiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.05]
heuristics/coefdiving/maxlpiterquot = 0.05

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/coefdiving/maxlpiterofs = 1000

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/coefdiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/coefdiving/maxdiveavgquot = 0

# maximal UBQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1], default: 0.1]
heuristics/coefdiving/maxdiveubquotnosol = 0.1

# maximal AVGQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/coefdiving/maxdiveavgquotnosol = 0

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/coefdiving/backtrack = TRUE

# priority of heuristic <crossover>
# [type: int, range: [-536870912,536870911], default: -1104000]
heuristics/crossover/priority = -1104000

# frequency for calling primal heuristic <crossover> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 30]
heuristics/crossover/freq = 30

# frequency offset for calling primal heuristic <crossover>
# [type: int, range: [0,2147483647], default: 0]
heuristics/crossover/freqofs = 0

# maximal depth level to call primal heuristic <crossover> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/crossover/maxdepth = -1

# number of nodes added to the contingent of the total nodes
# [type: longint, range: [0,9223372036854775807], default: 500]
heuristics/crossover/nodesofs = 500

# maximum number of nodes to regard in the subproblem
# [type: longint, range: [0,9223372036854775807], default: 5000]
heuristics/crossover/maxnodes = 5000

# minimum number of nodes required to start the subproblem
# [type: longint, range: [0,9223372036854775807], default: 50]
heuristics/crossover/minnodes = 50

# number of solutions to be taken into account
# [type: int, range: [2,2147483647], default: 3]
heuristics/crossover/nusedsols = 3

# number of nodes without incumbent change that heuristic should wait
# [type: longint, range: [0,9223372036854775807], default: 200]
heuristics/crossover/nwaitingnodes = 200

# contingent of sub problem nodes in relation to the number of nodes of the original problem
# [type: real, range: [0,1], default: 0.1]
heuristics/crossover/nodesquot = 0.1

# minimum percentage of integer variables that have to be fixed
# [type: real, range: [0,1], default: 0.666]
heuristics/crossover/minfixingrate = 0.666

# factor by which Crossover should at least improve the incumbent
# [type: real, range: [0,1], default: 0.01]
heuristics/crossover/minimprove = 0.01

# factor by which the limit on the number of LP depends on the node limit
# [type: real, range: [1,1.79769313486232e+308], default: 2]
heuristics/crossover/lplimfac = 2

# should the choice which sols to take be randomized?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/crossover/randomization = TRUE

# should the nwaitingnodes parameter be ignored at the root node?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/crossover/dontwaitatroot = FALSE

# should subproblem be created out of the rows in the LP rows?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/crossover/uselprows = FALSE

# if uselprows == FALSE, should all active cuts from cutpool be copied to constraints in subproblem?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/crossover/copycuts = TRUE

# should the subproblem be permuted to increase diversification?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/crossover/permute = FALSE

# priority of heuristic <dins>
# [type: int, range: [-536870912,536870911], default: -1105000]
heuristics/dins/priority = -1105000

# frequency for calling primal heuristic <dins> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/dins/freq = -1

# frequency offset for calling primal heuristic <dins>
# [type: int, range: [0,2147483647], default: 0]
heuristics/dins/freqofs = 0

# maximal depth level to call primal heuristic <dins> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/dins/maxdepth = -1

# number of nodes added to the contingent of the total nodes
# [type: longint, range: [0,9223372036854775807], default: 5000]
heuristics/dins/nodesofs = 5000

# contingent of sub problem nodes in relation to the number of nodes of the original problem
# [type: real, range: [0,1], default: 0.05]
heuristics/dins/nodesquot = 0.05

# minimum number of nodes required to start the subproblem
# [type: longint, range: [0,9223372036854775807], default: 50]
heuristics/dins/minnodes = 50

# number of pool-solutions to be checked for flag array update (for hard fixing of binary variables)
# [type: int, range: [1,2147483647], default: 5]
heuristics/dins/solnum = 5

# radius (using Manhattan metric) of the incumbent's neighborhood to be searched
# [type: int, range: [1,2147483647], default: 18]
heuristics/dins/neighborhoodsize = 18

# maximum number of nodes to regard in the subproblem
# [type: longint, range: [0,9223372036854775807], default: 5000]
heuristics/dins/maxnodes = 5000

# factor by which dins should at least improve the incumbent
# [type: real, range: [0,1], default: 0.01]
heuristics/dins/minimprove = 0.01

# number of nodes without incumbent change that heuristic should wait
# [type: longint, range: [0,9223372036854775807], default: 0]
heuristics/dins/nwaitingnodes = 0

# factor by which the limit on the number of LP depends on the node limit
# [type: real, range: [1,1.79769313486232e+308], default: 1.5]
heuristics/dins/lplimfac = 1.5

# minimum percentage of integer variables that have to be fixable
# [type: real, range: [0,1], default: 0.3]
heuristics/dins/minfixingrate = 0.3

# should subproblem be created out of the rows in the LP rows?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/dins/uselprows = FALSE

# if uselprows == FALSE, should all active cuts from cutpool be copied to constraints in subproblem?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/dins/copycuts = TRUE

# priority of heuristic <dualval>
# [type: int, range: [-536870912,536870911], default: 0]
heuristics/dualval/priority = 0

# frequency for calling primal heuristic <dualval> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/dualval/freq = -1

# frequency offset for calling primal heuristic <dualval>
# [type: int, range: [0,2147483647], default: 0]
heuristics/dualval/freqofs = 0

# maximal depth level to call primal heuristic <dualval> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/dualval/maxdepth = -1

# exit if objective doesn't improve
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/dualval/forceimprovements = FALSE

# add constraint to ensure that discrete vars are improving
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/dualval/onlycheaper = TRUE

# disable the heuristic if it was not called at a leaf of the B&B tree
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/dualval/onlyleaves = FALSE

# relax the indicator variables by introducing continuous copies
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/dualval/relaxindicators = FALSE

# relax the continous variables
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/dualval/relaxcontvars = FALSE

# verblevel of the heuristic, default is 0 to display nothing
# [type: int, range: [0,4], default: 0]
heuristics/dualval/heurverblevel = 0

# verblevel of the nlp solver, can be 0 or 1
# [type: int, range: [0,1], default: 0]
heuristics/dualval/nlpverblevel = 0

# number of ranks that should be displayed when the heuristic is called
# [type: int, range: [0,2147483647], default: 10]
heuristics/dualval/rankvalue = 10

# maximal number of recursive calls of the heuristic (if dynamicdepth is off)
# [type: int, range: [0,2147483647], default: 25]
heuristics/dualval/maxcalls = 25

# says if and how the recursion depth is computed at runtime
# [type: int, range: [0,1], default: 0]
heuristics/dualval/dynamicdepth = 0

# maximal number of variables that may have maximal rank, quit if there are more, turn off by setting -1
# [type: int, range: [-1,2147483647], default: 50]
heuristics/dualval/maxequalranks = 50

# minimal gap for which we still run the heuristic, if gap is less we return without doing anything
# [type: real, range: [0,100], default: 5]
heuristics/dualval/mingap = 5

# value added to objective of slack variables, must not be zero
# [type: real, range: [0.1,1e+20], default: 1]
heuristics/dualval/lambdaslack = 1

# scaling factor for the objective function
# [type: real, range: [0,1], default: 0]
heuristics/dualval/lambdaobj = 0

# priority of heuristic <feaspump>
# [type: int, range: [-536870912,536870911], default: -1000000]
heuristics/feaspump/priority = -1000000

# frequency for calling primal heuristic <feaspump> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 20]
heuristics/feaspump/freq = 20

# frequency offset for calling primal heuristic <feaspump>
# [type: int, range: [0,2147483647], default: 0]
heuristics/feaspump/freqofs = 0

# maximal depth level to call primal heuristic <feaspump> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/feaspump/maxdepth = -1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.01]
heuristics/feaspump/maxlpiterquot = 0.01

# factor by which the regard of the objective is decreased in each round, 1.0 for dynamic
# [type: real, range: [0,1], default: 0.1]
heuristics/feaspump/objfactor = 0.1

# initial weight of the objective function in the convex combination
# [type: real, range: [0,1], default: 1]
heuristics/feaspump/alpha = 1

# threshold difference for the convex parameter to perform perturbation
# [type: real, range: [0,1], default: 1]
heuristics/feaspump/alphadiff = 1

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/feaspump/maxlpiterofs = 1000

# total number of feasible solutions found up to which heuristic is called (-1: no limit)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/feaspump/maxsols = 10

# maximal number of pumping loops (-1: no limit)
# [type: int, range: [-1,2147483647], default: 10000]
heuristics/feaspump/maxloops = 10000

# maximal number of pumping rounds without fractionality improvement (-1: no limit)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/feaspump/maxstallloops = 10

# minimum number of random variables to flip, if a 1-cycle is encountered
# [type: int, range: [1,2147483647], default: 10]
heuristics/feaspump/minflips = 10

# maximum length of cycles to be checked explicitly in each round
# [type: int, range: [1,100], default: 3]
heuristics/feaspump/cyclelength = 3

# number of iterations until a random perturbation is forced
# [type: int, range: [1,2147483647], default: 100]
heuristics/feaspump/perturbfreq = 100

# radius (using Manhattan metric) of the neighborhood to be searched in stage 3
# [type: int, range: [1,2147483647], default: 18]
heuristics/feaspump/neighborhoodsize = 18

# should the feasibility pump be called at root node before cut separation?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/feaspump/beforecuts = TRUE

# should an iterative round-and-propagate scheme be used to find the integral points?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/feaspump/usefp20 = FALSE

# should a random perturbation be performed if a feasible solution was found?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/feaspump/pertsolfound = TRUE

# should we solve a local branching sub-MIP if no solution could be found?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/feaspump/stage3 = FALSE

# should all active cuts from cutpool be copied to constraints in subproblem?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/feaspump/copycuts = TRUE

# priority of heuristic <fixandinfer>
# [type: int, range: [-536870912,536870911], default: -500000]
heuristics/fixandinfer/priority = -500000

# frequency for calling primal heuristic <fixandinfer> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/fixandinfer/freq = -1

# frequency offset for calling primal heuristic <fixandinfer>
# [type: int, range: [0,2147483647], default: 0]
heuristics/fixandinfer/freqofs = 0

# maximal depth level to call primal heuristic <fixandinfer> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/fixandinfer/maxdepth = -1

# maximal number of propagation rounds in probing subproblems (-1: no limit, 0: auto)
# [type: int, range: [-1,2147483647], default: 0]
heuristics/fixandinfer/proprounds = 0

# minimal number of fixings to apply before dive may be aborted
# [type: int, range: [0,2147483647], default: 100]
heuristics/fixandinfer/minfixings = 100

# priority of heuristic <fracdiving>
# [type: int, range: [-536870912,536870911], default: -1003000]
heuristics/fracdiving/priority = -1003000

# frequency for calling primal heuristic <fracdiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/fracdiving/freq = 10

# frequency offset for calling primal heuristic <fracdiving>
# [type: int, range: [0,2147483647], default: 3]
heuristics/fracdiving/freqofs = 3

# maximal depth level to call primal heuristic <fracdiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/fracdiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/fracdiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/fracdiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.05]
heuristics/fracdiving/maxlpiterquot = 0.05

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/fracdiving/maxlpiterofs = 1000

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/fracdiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/fracdiving/maxdiveavgquot = 0

# maximal UBQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1], default: 0.1]
heuristics/fracdiving/maxdiveubquotnosol = 0.1

# maximal AVGQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/fracdiving/maxdiveavgquotnosol = 0

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/fracdiving/backtrack = TRUE

# priority of heuristic <guideddiving>
# [type: int, range: [-536870912,536870911], default: -1007000]
heuristics/guideddiving/priority = -1007000

# frequency for calling primal heuristic <guideddiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/guideddiving/freq = 10

# frequency offset for calling primal heuristic <guideddiving>
# [type: int, range: [0,2147483647], default: 7]
heuristics/guideddiving/freqofs = 7

# maximal depth level to call primal heuristic <guideddiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/guideddiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/guideddiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/guideddiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.05]
heuristics/guideddiving/maxlpiterquot = 0.05

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/guideddiving/maxlpiterofs = 1000

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/guideddiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/guideddiving/maxdiveavgquot = 0

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/guideddiving/backtrack = TRUE

# priority of heuristic <intdiving>
# [type: int, range: [-536870912,536870911], default: -1003500]
heuristics/intdiving/priority = -1003500

# frequency for calling primal heuristic <intdiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/intdiving/freq = -1

# frequency offset for calling primal heuristic <intdiving>
# [type: int, range: [0,2147483647], default: 9]
heuristics/intdiving/freqofs = 9

# maximal depth level to call primal heuristic <intdiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/intdiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/intdiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/intdiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.05]
heuristics/intdiving/maxlpiterquot = 0.05

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/intdiving/maxlpiterofs = 1000

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/intdiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/intdiving/maxdiveavgquot = 0

# maximal UBQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1], default: 0.1]
heuristics/intdiving/maxdiveubquotnosol = 0.1

# maximal AVGQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/intdiving/maxdiveavgquotnosol = 0

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/intdiving/backtrack = TRUE

# priority of heuristic <intshifting>
# [type: int, range: [-536870912,536870911], default: -10000]
heuristics/intshifting/priority = -10000

# frequency for calling primal heuristic <intshifting> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/intshifting/freq = 10

# frequency offset for calling primal heuristic <intshifting>
# [type: int, range: [0,2147483647], default: 0]
heuristics/intshifting/freqofs = 0

# maximal depth level to call primal heuristic <intshifting> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/intshifting/maxdepth = -1

# priority of heuristic <linesearchdiving>
# [type: int, range: [-536870912,536870911], default: -1006000]
heuristics/linesearchdiving/priority = -1006000

# frequency for calling primal heuristic <linesearchdiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/linesearchdiving/freq = 10

# frequency offset for calling primal heuristic <linesearchdiving>
# [type: int, range: [0,2147483647], default: 6]
heuristics/linesearchdiving/freqofs = 6

# maximal depth level to call primal heuristic <linesearchdiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/linesearchdiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/linesearchdiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/linesearchdiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.05]
heuristics/linesearchdiving/maxlpiterquot = 0.05

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/linesearchdiving/maxlpiterofs = 1000

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/linesearchdiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/linesearchdiving/maxdiveavgquot = 0

# maximal UBQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1], default: 0.1]
heuristics/linesearchdiving/maxdiveubquotnosol = 0.1

# maximal AVGQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/linesearchdiving/maxdiveavgquotnosol = 0

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/linesearchdiving/backtrack = TRUE

# priority of heuristic <localbranching>
# [type: int, range: [-536870912,536870911], default: -1102000]
heuristics/localbranching/priority = -1102000

# frequency for calling primal heuristic <localbranching> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/localbranching/freq = -1

# frequency offset for calling primal heuristic <localbranching>
# [type: int, range: [0,2147483647], default: 0]
heuristics/localbranching/freqofs = 0

# maximal depth level to call primal heuristic <localbranching> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/localbranching/maxdepth = -1

# number of nodes added to the contingent of the total nodes
# [type: int, range: [0,2147483647], default: 1000]
heuristics/localbranching/nodesofs = 1000

# radius (using Manhattan metric) of the incumbent's neighborhood to be searched
# [type: int, range: [1,2147483647], default: 18]
heuristics/localbranching/neighborhoodsize = 18

# contingent of sub problem nodes in relation to the number of nodes of the original problem
# [type: real, range: [0,1], default: 0.05]
heuristics/localbranching/nodesquot = 0.05

# factor by which the limit on the number of LP depends on the node limit
# [type: real, range: [1,1.79769313486232e+308], default: 1.5]
heuristics/localbranching/lplimfac = 1.5

# minimum number of nodes required to start the subproblem
# [type: int, range: [0,2147483647], default: 1000]
heuristics/localbranching/minnodes = 1000

# maximum number of nodes to regard in the subproblem
# [type: int, range: [0,2147483647], default: 10000]
heuristics/localbranching/maxnodes = 10000

# number of nodes without incumbent change that heuristic should wait
# [type: int, range: [0,2147483647], default: 200]
heuristics/localbranching/nwaitingnodes = 200

# factor by which localbranching should at least improve the incumbent
# [type: real, range: [0,1], default: 0.01]
heuristics/localbranching/minimprove = 0.01

# should subproblem be created out of the rows in the LP rows?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/localbranching/uselprows = FALSE

# if uselprows == FALSE, should all active cuts from cutpool be copied to constraints in subproblem?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/localbranching/copycuts = TRUE

# priority of heuristic <mutation>
# [type: int, range: [-536870912,536870911], default: -1103000]
heuristics/mutation/priority = -1103000

# frequency for calling primal heuristic <mutation> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/mutation/freq = -1

# frequency offset for calling primal heuristic <mutation>
# [type: int, range: [0,2147483647], default: 8]
heuristics/mutation/freqofs = 8

# maximal depth level to call primal heuristic <mutation> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/mutation/maxdepth = -1

# number of nodes added to the contingent of the total nodes
# [type: int, range: [0,2147483647], default: 500]
heuristics/mutation/nodesofs = 500

# maximum number of nodes to regard in the subproblem
# [type: int, range: [0,2147483647], default: 5000]
heuristics/mutation/maxnodes = 5000

# minimum number of nodes required to start the subproblem
# [type: int, range: [0,2147483647], default: 500]
heuristics/mutation/minnodes = 500

# number of nodes without incumbent change that heuristic should wait
# [type: int, range: [0,2147483647], default: 200]
heuristics/mutation/nwaitingnodes = 200

# contingent of sub problem nodes in relation to the number of nodes of the original problem
# [type: real, range: [0,1], default: 0.1]
heuristics/mutation/nodesquot = 0.1

# percentage of integer variables that have to be fixed
# [type: real, range: [1e-06,0.999999], default: 0.8]
heuristics/mutation/minfixingrate = 0.8

# factor by which mutation should at least improve the incumbent
# [type: real, range: [0,1], default: 0.01]
heuristics/mutation/minimprove = 0.01

# should subproblem be created out of the rows in the LP rows?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/mutation/uselprows = FALSE

# if uselprows == FALSE, should all active cuts from cutpool be copied to constraints in subproblem?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/mutation/copycuts = TRUE

# priority of heuristic <nlpdiving>
# [type: int, range: [-536870912,536870911], default: -1003000]
heuristics/nlpdiving/priority = -1003000

# frequency for calling primal heuristic <nlpdiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/nlpdiving/freq = 10

# frequency offset for calling primal heuristic <nlpdiving>
# [type: int, range: [0,2147483647], default: 3]
heuristics/nlpdiving/freqofs = 3

# maximal depth level to call primal heuristic <nlpdiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/nlpdiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/nlpdiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/nlpdiving/maxreldepth = 1

# minimial absolute number of allowed NLP iterations
# [type: int, range: [0,2147483647], default: 200]
heuristics/nlpdiving/maxnlpiterabs = 200

# additional allowed number of NLP iterations relative to successfully found solutions
# [type: int, range: [0,2147483647], default: 10]
heuristics/nlpdiving/maxnlpiterrel = 10

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/nlpdiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/nlpdiving/maxdiveavgquot = 0

# maximal UBQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1], default: 0.1]
heuristics/nlpdiving/maxdiveubquotnosol = 0.1

# maximal AVGQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/nlpdiving/maxdiveavgquotnosol = 0

# maximal number of NLPs with feasible solution to solve during one dive
# [type: int, range: [1,2147483647], default: 10]
heuristics/nlpdiving/maxfeasnlps = 10

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/nlpdiving/backtrack = TRUE

# should the LP relaxation be solved before the NLP relaxation?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/nlpdiving/lp = FALSE

# prefer variables that are also fractional in LP solution?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/nlpdiving/preferlpfracs = FALSE

# heuristic will not run if less then this percentage of calls succeeded (0.0: no limit)
# [type: real, range: [0,1], default: 0.1]
heuristics/nlpdiving/minsuccquot = 0.1

# percentage of fractional variables that should be fixed before the next NLP solve
# [type: real, range: [0,1], default: 0.2]
heuristics/nlpdiving/fixquot = 0.2

# should variables in a minimal cover be preferred?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/nlpdiving/prefercover = TRUE

# should a sub-MIP be solved if all cover variables are fixed?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/nlpdiving/solvesubmip = FALSE

# should the NLP solver stop early if it converges slow?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/nlpdiving/nlpfastfail = TRUE

# which point should be used as starting point for the NLP solver? ('n'one, last 'f'easible, from dive's'tart)
# [type: char, range: {fns}, default: s]
heuristics/nlpdiving/nlpstart = s

# which variable selection should be used? ('f'ractionality, 'c'oefficient, 'p'seudocost, 'g'uided, 'd'ouble, 'v'eclen)
# [type: char, range: {fcpgdv}, default: d]
heuristics/nlpdiving/varselrule = d

# priority of heuristic <objpscostdiving>
# [type: int, range: [-536870912,536870911], default: -1004000]
heuristics/objpscostdiving/priority = -1004000

# frequency for calling primal heuristic <objpscostdiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 20]
heuristics/objpscostdiving/freq = 20

# frequency offset for calling primal heuristic <objpscostdiving>
# [type: int, range: [0,2147483647], default: 4]
heuristics/objpscostdiving/freqofs = 4

# maximal depth level to call primal heuristic <objpscostdiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/objpscostdiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/objpscostdiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/objpscostdiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to total iteration number
# [type: real, range: [0,1], default: 0.01]
heuristics/objpscostdiving/maxlpiterquot = 0.01

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/objpscostdiving/maxlpiterofs = 1000

# total number of feasible solutions found up to which heuristic is called (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/objpscostdiving/maxsols = -1

# maximal diving depth: number of binary/integer variables times depthfac
# [type: real, range: [0,1.79769313486232e+308], default: 0.5]
heuristics/objpscostdiving/depthfac = 0.5

# maximal diving depth factor if no feasible solution was found yet
# [type: real, range: [0,1.79769313486232e+308], default: 2]
heuristics/objpscostdiving/depthfacnosol = 2

# priority of heuristic <octane>
# [type: int, range: [-536870912,536870911], default: -1008000]
heuristics/octane/priority = -1008000

# frequency for calling primal heuristic <octane> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/octane/freq = -1

# frequency offset for calling primal heuristic <octane>
# [type: int, range: [0,2147483647], default: 0]
heuristics/octane/freqofs = 0

# maximal depth level to call primal heuristic <octane> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/octane/maxdepth = -1

# number of 0-1-points to be tested as possible solutions by OCTANE
# [type: int, range: [1,2147483647], default: 100]
heuristics/octane/fmax = 100

# number of 0-1-points to be tested at first whether they violate a common row
# [type: int, range: [1,2147483647], default: 10]
heuristics/octane/ffirst = 10

# execute OCTANE only in the space of fractional variables (TRUE) or in the full space?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/octane/usefracspace = TRUE

# should the inner normal of the objective be used as one ray direction?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/octane/useobjray = TRUE

# should the average of the basic cone be used as one ray direction?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/octane/useavgray = TRUE

# should the difference between the root solution and the current LP solution be used as one ray direction?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/octane/usediffray = FALSE

# should the weighted average of the basic cone be used as one ray direction?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/octane/useavgwgtray = TRUE

# should the weighted average of the nonbasic cone be used as one ray direction?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/octane/useavgnbray = TRUE

# priority of heuristic <oneopt>
# [type: int, range: [-536870912,536870911], default: -20000]
heuristics/oneopt/priority = -20000

# frequency for calling primal heuristic <oneopt> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 1]
heuristics/oneopt/freq = 1

# frequency offset for calling primal heuristic <oneopt>
# [type: int, range: [0,2147483647], default: 0]
heuristics/oneopt/freqofs = 0

# maximal depth level to call primal heuristic <oneopt> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/oneopt/maxdepth = -1

# should the objective be weighted with the potential shifting value when sorting the shifting candidates?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/oneopt/weightedobj = TRUE

# should the heuristic be called before and during the root node?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/oneopt/duringroot = TRUE

# should the construction of the LP be forced even if LP solving is deactivated?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/oneopt/forcelpconstruction = FALSE

# should the heuristic be called before presolving?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/oneopt/beforepresol = FALSE

# priority of heuristic <proximity>
# [type: int, range: [-536870912,536870911], default: -2000000]
heuristics/proximity/priority = -2000000

# frequency for calling primal heuristic <proximity> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/proximity/freq = -1

# frequency offset for calling primal heuristic <proximity>
# [type: int, range: [0,2147483647], default: 0]
heuristics/proximity/freqofs = 0

# maximal depth level to call primal heuristic <proximity> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/proximity/maxdepth = -1

# should subproblem be constructed based on LP row information?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/proximity/uselprows = FALSE

# should the heuristic immediately run again on its newly found solution?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/proximity/restart = TRUE

# should the heuristic solve a final LP in case of continuous objective variables?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/proximity/usefinallp = FALSE

# maximum number of nodes to regard in the subproblem
# [type: longint, range: [0,9223372036854775807], default: 10000]
heuristics/proximity/maxnodes = 10000

# number of nodes added to the contingent of the total nodes
# [type: longint, range: [0,9223372036854775807], default: 50]
heuristics/proximity/nodesofs = 50

# minimum number of nodes required to start the subproblem
# [type: longint, range: [0,9223372036854775807], default: 1]
heuristics/proximity/minnodes = 1

# maximum number of LP iterations to be performed in the subproblem
# [type: longint, range: [-1,9223372036854775807], default: 100000]
heuristics/proximity/maxlpiters = 100000

# minimum number of LP iterations performed in subproblem
# [type: longint, range: [0,9223372036854775807], default: 200]
heuristics/proximity/minlpiters = 200

# waiting nodes since last incumbent before heuristic is executed
# [type: longint, range: [0,9223372036854775807], default: 100]
heuristics/proximity/waitingnodes = 100

# factor by which proximity should at least improve the incumbent
# [type: real, range: [0,1], default: 0.02]
heuristics/proximity/minimprove = 0.02

# sub-MIP node limit w.r.t number of original nodes
# [type: real, range: [0,1e+20], default: 0.1]
heuristics/proximity/nodesquot = 0.1

# threshold for percentage of binary variables required to start
# [type: real, range: [0,1], default: 0.1]
heuristics/proximity/binvarquot = 0.1

# quotient of sub-MIP LP iterations with respect to LP iterations so far
# [type: real, range: [0,1], default: 0.2]
heuristics/proximity/lpitersquot = 0.2

# minimum primal-dual gap for which the heuristic is executed
# [type: real, range: [0,1e+20], default: 0.01]
heuristics/proximity/mingap = 0.01

# priority of heuristic <pscostdiving>
# [type: int, range: [-536870912,536870911], default: -1002000]
heuristics/pscostdiving/priority = -1002000

# frequency for calling primal heuristic <pscostdiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/pscostdiving/freq = 10

# frequency offset for calling primal heuristic <pscostdiving>
# [type: int, range: [0,2147483647], default: 2]
heuristics/pscostdiving/freqofs = 2

# maximal depth level to call primal heuristic <pscostdiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/pscostdiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/pscostdiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/pscostdiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.05]
heuristics/pscostdiving/maxlpiterquot = 0.05

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/pscostdiving/maxlpiterofs = 1000

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/pscostdiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/pscostdiving/maxdiveavgquot = 0

# maximal UBQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1], default: 0.1]
heuristics/pscostdiving/maxdiveubquotnosol = 0.1

# maximal AVGQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/pscostdiving/maxdiveavgquotnosol = 0

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/pscostdiving/backtrack = TRUE

# priority of heuristic <randrounding>
# [type: int, range: [-536870912,536870911], default: -200]
heuristics/randrounding/priority = -200

# frequency for calling primal heuristic <randrounding> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 20]
heuristics/randrounding/freq = 20

# frequency offset for calling primal heuristic <randrounding>
# [type: int, range: [0,2147483647], default: 0]
heuristics/randrounding/freqofs = 0

# maximal depth level to call primal heuristic <randrounding> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/randrounding/maxdepth = -1

# should the heuristic only be called once per node?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/randrounding/oncepernode = FALSE

# should the heuristic apply the variable lock strategy of simple rounding, if possible?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/randrounding/usesimplerounding = FALSE

# should the probing part of the heuristic be applied exclusively at the root node?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/randrounding/propagateonlyroot = TRUE

# limit of rounds for each propagation call
# [type: int, range: [-1,2147483647], default: 1]
heuristics/randrounding/maxproprounds = 1

# priority of heuristic <rens>
# [type: int, range: [-536870912,536870911], default: -1100000]
heuristics/rens/priority = -1100000

# frequency for calling primal heuristic <rens> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 0]
heuristics/rens/freq = 0

# frequency offset for calling primal heuristic <rens>
# [type: int, range: [0,2147483647], default: 0]
heuristics/rens/freqofs = 0

# maximal depth level to call primal heuristic <rens> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/rens/maxdepth = -1

# minimum percentage of integer variables that have to be fixable
# [type: real, range: [0,1], default: 0.5]
heuristics/rens/minfixingrate = 0.5

# maximum number of nodes to regard in the subproblem
# [type: longint, range: [0,9223372036854775807], default: 5000]
heuristics/rens/maxnodes = 5000

# number of nodes added to the contingent of the total nodes
# [type: longint, range: [0,9223372036854775807], default: 500]
heuristics/rens/nodesofs = 500

# minimum number of nodes required to start the subproblem
# [type: longint, range: [0,9223372036854775807], default: 50]
heuristics/rens/minnodes = 50

# contingent of sub problem nodes in relation to the number of nodes of the original problem
# [type: real, range: [0,1], default: 0.1]
heuristics/rens/nodesquot = 0.1

# factor by which RENS should at least improve the incumbent
# [type: real, range: [0,1], default: 0.01]
heuristics/rens/minimprove = 0.01

# factor by which the limit on the number of LP depends on the node limit
# [type: real, range: [1,1.79769313486232e+308], default: 2]
heuristics/rens/lplimfac = 2

# solution that is used for fixing values ('l'p relaxation, 'n'lp relaxation)
# [type: char, range: {nl}, default: l]
heuristics/rens/startsol = l

# should general integers get binary bounds [floor(.),ceil(.)] ?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/rens/binarybounds = TRUE

# should subproblem be created out of the rows in the LP rows?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/rens/uselprows = FALSE

# if uselprows == FALSE, should all active cuts from cutpool be copied to constraints in subproblem?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/rens/copycuts = TRUE

# should the RENS sub-CIP get its own full time limit? This is only for tesing and not recommended!
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/rens/extratime = FALSE

# should all subproblem solutions be added to the original SCIP?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/rens/addallsols = FALSE

# should the RENS sub-CIP be solved with cuts, conflicts, strong branching,... This is only for tesing and not recommended!
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/rens/fullscale = FALSE

# priority of heuristic <rins>
# [type: int, range: [-536870912,536870911], default: -1101000]
heuristics/rins/priority = -1101000

# frequency for calling primal heuristic <rins> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 25]
heuristics/rins/freq = 25

# frequency offset for calling primal heuristic <rins>
# [type: int, range: [0,2147483647], default: 0]
heuristics/rins/freqofs = 0

# maximal depth level to call primal heuristic <rins> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/rins/maxdepth = -1

# number of nodes added to the contingent of the total nodes
# [type: int, range: [0,2147483647], default: 500]
heuristics/rins/nodesofs = 500

# maximum number of nodes to regard in the subproblem
# [type: int, range: [0,2147483647], default: 5000]
heuristics/rins/maxnodes = 5000

# minimum number of nodes required to start the subproblem
# [type: int, range: [0,2147483647], default: 50]
heuristics/rins/minnodes = 50

# contingent of sub problem nodes in relation to the number of nodes of the original problem
# [type: real, range: [0,1], default: 0.1]
heuristics/rins/nodesquot = 0.1

# number of nodes without incumbent change that heuristic should wait
# [type: int, range: [0,2147483647], default: 200]
heuristics/rins/nwaitingnodes = 200

# factor by which rins should at least improve the incumbent
# [type: real, range: [0,1], default: 0.01]
heuristics/rins/minimprove = 0.01

# minimum percentage of integer variables that have to be fixed
# [type: real, range: [0,1], default: 0.3]
heuristics/rins/minfixingrate = 0.3

# factor by which the limit on the number of LP depends on the node limit
# [type: real, range: [1,1.79769313486232e+308], default: 2]
heuristics/rins/lplimfac = 2

# should subproblem be created out of the rows in the LP rows?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/rins/uselprows = FALSE

# if uselprows == FALSE, should all active cuts from cutpool be copied to constraints in subproblem?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/rins/copycuts = TRUE

# priority of heuristic <rootsoldiving>
# [type: int, range: [-536870912,536870911], default: -1005000]
heuristics/rootsoldiving/priority = -1005000

# frequency for calling primal heuristic <rootsoldiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 20]
heuristics/rootsoldiving/freq = 20

# frequency offset for calling primal heuristic <rootsoldiving>
# [type: int, range: [0,2147483647], default: 5]
heuristics/rootsoldiving/freqofs = 5

# maximal depth level to call primal heuristic <rootsoldiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/rootsoldiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/rootsoldiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/rootsoldiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.01]
heuristics/rootsoldiving/maxlpiterquot = 0.01

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/rootsoldiving/maxlpiterofs = 1000

# total number of feasible solutions found up to which heuristic is called (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/rootsoldiving/maxsols = -1

# maximal diving depth: number of binary/integer variables times depthfac
# [type: real, range: [0,1.79769313486232e+308], default: 0.5]
heuristics/rootsoldiving/depthfac = 0.5

# maximal diving depth factor if no feasible solution was found yet
# [type: real, range: [0,1.79769313486232e+308], default: 2]
heuristics/rootsoldiving/depthfacnosol = 2

# soft rounding factor to fade out objective coefficients
# [type: real, range: [0,1], default: 0.9]
heuristics/rootsoldiving/alpha = 0.9

# priority of heuristic <rounding>
# [type: int, range: [-536870912,536870911], default: -1000]
heuristics/rounding/priority = -1000

# frequency for calling primal heuristic <rounding> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 1]
heuristics/rounding/freq = 1

# frequency offset for calling primal heuristic <rounding>
# [type: int, range: [0,2147483647], default: 0]
heuristics/rounding/freqofs = 0

# maximal depth level to call primal heuristic <rounding> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/rounding/maxdepth = -1

# number of calls per found solution that are considered as standard success, a higher factor causes the heuristic to be called more often
# [type: int, range: [-1,2147483647], default: 100]
heuristics/rounding/successfactor = 100

# should the heuristic only be called once per node?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/rounding/oncepernode = FALSE

# priority of heuristic <shiftandpropagate>
# [type: int, range: [-536870912,536870911], default: 1000]
heuristics/shiftandpropagate/priority = 1000

# frequency for calling primal heuristic <shiftandpropagate> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 0]
heuristics/shiftandpropagate/freq = 0

# frequency offset for calling primal heuristic <shiftandpropagate>
# [type: int, range: [0,2147483647], default: 0]
heuristics/shiftandpropagate/freqofs = 0

# maximal depth level to call primal heuristic <shiftandpropagate> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/shiftandpropagate/maxdepth = -1

# The number of propagation rounds used for each propagation
# [type: int, range: [-1,1000], default: 10]
heuristics/shiftandpropagate/nproprounds = 10

# Should continuous variables be relaxed?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/shiftandpropagate/relax = TRUE

# Should domains be reduced by probing?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/shiftandpropagate/probing = TRUE

# Should heuristic only be executed if no primal solution was found, yet?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/shiftandpropagate/onlywithoutsol = TRUE

# The number of cutoffs before heuristic stops
# [type: int, range: [-1,1000000], default: 15]
heuristics/shiftandpropagate/cutoffbreaker = 15

# the key for variable sorting: (n)orms down, norms (u)p, (v)iolations down, viola(t)ions up, or (r)andom
# [type: char, range: {nrtuv}, default: v]
heuristics/shiftandpropagate/sortkey = v

# Should variables be sorted for the heuristic?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/shiftandpropagate/sortvars = TRUE

# should variable statistics be collected during probing?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/shiftandpropagate/collectstats = TRUE

# Should the heuristic stop calculating optimal shift values when no more rows are violated?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/shiftandpropagate/stopafterfeasible = TRUE

# Should binary variables be shifted first?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/shiftandpropagate/preferbinaries = TRUE

# should variables with a zero shifting value be delayed instead of being fixed?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/shiftandpropagate/nozerofixing = FALSE

# should binary variables with no locks in one direction be fixed to that direction?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/shiftandpropagate/fixbinlocks = TRUE

# should coefficients and left/right hand sides be normalized by max row coeff?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/shiftandpropagate/normalize = TRUE

# should row weight be increased every time the row is violated?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/shiftandpropagate/updateweights = FALSE

# should implicit integer variables be treated as continuous variables?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/shiftandpropagate/impliscontinuous = TRUE

# priority of heuristic <shifting>
# [type: int, range: [-536870912,536870911], default: -5000]
heuristics/shifting/priority = -5000

# frequency for calling primal heuristic <shifting> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/shifting/freq = 10

# frequency offset for calling primal heuristic <shifting>
# [type: int, range: [0,2147483647], default: 0]
heuristics/shifting/freqofs = 0

# maximal depth level to call primal heuristic <shifting> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/shifting/maxdepth = -1

# priority of heuristic <subnlp>
# [type: int, range: [-536870912,536870911], default: -2000000]
heuristics/subnlp/priority = -2000000

# frequency for calling primal heuristic <subnlp> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 1]
heuristics/subnlp/freq = 1

# frequency offset for calling primal heuristic <subnlp>
# [type: int, range: [0,2147483647], default: 0]
heuristics/subnlp/freqofs = 0

# maximal depth level to call primal heuristic <subnlp> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/subnlp/maxdepth = -1

# verbosity level of NLP solver
# [type: int, range: [0,2147483647], default: 0]
heuristics/subnlp/nlpverblevel = 0

# iteration limit of NLP solver; 0 to use solver default
# [type: int, range: [0,2147483647], default: 0]
heuristics/subnlp/nlpiterlimit = 0

# time limit of NLP solver; 0 to use solver default
# [type: real, range: [0,1e+20], default: 0]
heuristics/subnlp/nlptimelimit = 0

# name of an NLP solver specific options file
# [type: string, default: ""]
heuristics/subnlp/nlpoptfile = ""

# if SCIP does not accept a NLP feasible solution, resolve NLP with feas. tolerance reduced by this factor (set to 1.0 to turn off resolve)
# [type: real, range: [0,1], default: 0.001]
heuristics/subnlp/resolvetolfactor = 0.001

# should the NLP resolve be started from the original starting point or the infeasible solution?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/subnlp/resolvefromscratch = TRUE

# number of iterations added to the contingent of the total number of iterations
# [type: int, range: [0,2147483647], default: 500]
heuristics/subnlp/iteroffset = 500

# contingent of NLP iterations in relation to the number of nodes in SCIP
# [type: real, range: [0,1e+20], default: 0.1]
heuristics/subnlp/iterquotient = 0.1

# contingent of NLP iterations in relation to the number of nodes in SCIP
# [type: int, range: [0,2147483647], default: 300]
heuristics/subnlp/itermin = 300

# whether to run NLP heuristic always if starting point available (does not use iteroffset,iterquot,itermin)
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/subnlp/runalways = FALSE

# factor by which NLP heuristic should at least improve the incumbent
# [type: real, range: [0,1], default: 0.01]
heuristics/subnlp/minimprove = 0.01

# limit on number of presolve rounds in sub-SCIP (-1 for unlimited, 0 for no presolve)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/subnlp/maxpresolverounds = -1

# whether to add constraints that forbid specific fixings that turned out to be infeasible
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/subnlp/forbidfixings = TRUE

# whether to keep SCIP copy or to create new copy each time heuristic is applied
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/subnlp/keepcopy = TRUE

# priority of heuristic <trivial>
# [type: int, range: [-536870912,536870911], default: 10000]
heuristics/trivial/priority = 10000

# frequency for calling primal heuristic <trivial> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 0]
heuristics/trivial/freq = 0

# frequency offset for calling primal heuristic <trivial>
# [type: int, range: [0,2147483647], default: 0]
heuristics/trivial/freqofs = 0

# maximal depth level to call primal heuristic <trivial> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/trivial/maxdepth = -1

# priority of heuristic <trysol>
# [type: int, range: [-536870912,536870911], default: -3000000]
heuristics/trysol/priority = -3000000

# frequency for calling primal heuristic <trysol> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 1]
heuristics/trysol/freq = 1

# frequency offset for calling primal heuristic <trysol>
# [type: int, range: [0,2147483647], default: 0]
heuristics/trysol/freqofs = 0

# maximal depth level to call primal heuristic <trysol> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/trysol/maxdepth = -1

# priority of heuristic <twoopt>
# [type: int, range: [-536870912,536870911], default: -20100]
heuristics/twoopt/priority = -20100

# frequency for calling primal heuristic <twoopt> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/twoopt/freq = -1

# frequency offset for calling primal heuristic <twoopt>
# [type: int, range: [0,2147483647], default: 0]
heuristics/twoopt/freqofs = 0

# maximal depth level to call primal heuristic <twoopt> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/twoopt/maxdepth = -1

#  Should Integer-2-Optimization be applied or not?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/twoopt/intopt = FALSE

# user parameter to determine number of nodes to wait after last best solution before calling heuristic
# [type: int, range: [0,10000], default: 0]
heuristics/twoopt/waitingnodes = 0

# maximum number of slaves for one master variable
# [type: int, range: [-1,1000000], default: 199]
heuristics/twoopt/maxnslaves = 199

# parameter to determine the percentage of rows two variables have to share before they are considered equal
# [type: real, range: [0,1], default: 0.5]
heuristics/twoopt/matchingrate = 0.5

# priority of heuristic <undercover>
# [type: int, range: [-536870912,536870911], default: -1110000]
heuristics/undercover/priority = -1110000

# frequency for calling primal heuristic <undercover> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 0]
heuristics/undercover/freq = 0

# frequency offset for calling primal heuristic <undercover>
# [type: int, range: [0,2147483647], default: 0]
heuristics/undercover/freqofs = 0

# maximal depth level to call primal heuristic <undercover> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/undercover/maxdepth = -1

# prioritized sequence of fixing values used ('l'p relaxation, 'n'lp relaxation, 'i'ncumbent solution)
# [type: string, default: "li"]
heuristics/undercover/fixingalts = "li"

# maximum number of nodes to regard in the subproblem
# [type: longint, range: [0,9223372036854775807], default: 500]
heuristics/undercover/maxnodes = 500

# minimum number of nodes required to start the subproblem
# [type: longint, range: [0,9223372036854775807], default: 500]
heuristics/undercover/minnodes = 500

# number of nodes added to the contingent of the total nodes
# [type: longint, range: [0,9223372036854775807], default: 500]
heuristics/undercover/nodesofs = 500

# weight for conflict score in fixing order
# [type: real, range: [-1.79769313486232e+308,1.79769313486232e+308], default: 1000]
heuristics/undercover/conflictweight = 1000

# weight for cutoff score in fixing order
# [type: real, range: [0,1.79769313486232e+308], default: 1]
heuristics/undercover/cutoffweight = 1

# weight for inference score in fixing order
# [type: real, range: [-1.79769313486232e+308,1.79769313486232e+308], default: 1]
heuristics/undercover/inferenceweight = 1

# maximum coversize (as fraction of total number of variables)
# [type: real, range: [0,1], default: 1]
heuristics/undercover/maxcoversizevars = 1

# maximum coversize maximum coversize (as ratio to the percentage of non-affected constraints)
# [type: real, range: [0,1.79769313486232e+308], default: 1.79769313486232e+308]
heuristics/undercover/maxcoversizeconss = 1.79769313486232e+308

# minimum percentage of nonlinear constraints in the original problem
# [type: real, range: [0,1], default: 0.15]
heuristics/undercover/mincoveredrel = 0.15

# factor by which the heuristic should at least improve the incumbent
# [type: real, range: [-1,1], default: 0]
heuristics/undercover/minimprove = 0

# contingent of sub problem nodes in relation to the number of nodes of the original problem
# [type: real, range: [0,1], default: 0.1]
heuristics/undercover/nodesquot = 0.1

# fraction of covering variables in the last cover which need to change their value when recovering
# [type: real, range: [0,1], default: 0.9]
heuristics/undercover/recoverdiv = 0.9

# minimum number of nonlinear constraints in the original problem
# [type: int, range: [0,2147483647], default: 5]
heuristics/undercover/mincoveredabs = 5

# maximum number of backtracks in fix-and-propagate
# [type: int, range: [0,2147483647], default: 6]
heuristics/undercover/maxbacktracks = 6

# maximum number of recoverings
# [type: int, range: [0,2147483647], default: 0]
heuristics/undercover/maxrecovers = 0

# maximum number of reorderings of the fixing order
# [type: int, range: [0,2147483647], default: 1]
heuristics/undercover/maxreorders = 1

# objective function of the covering problem (influenced nonlinear 'c'onstraints/'t'erms, 'd'omain size, 'l'ocks, 'm'in of up/down locks, 'u'nit penalties)
# [type: char, range: {cdlmtu}, default: u]
heuristics/undercover/coveringobj = u

# order in which variables should be fixed (increasing 'C'onflict score, decreasing 'c'onflict score, increasing 'V'ariable index, decreasing 'v'ariable index
# [type: char, range: {CcVv}, default: v]
heuristics/undercover/fixingorder = v

# should the heuristic be called at root node before cut separation?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/undercover/beforecuts = TRUE

# should integer variables in the cover be fixed first?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/undercover/fixintfirst = FALSE

# shall LP values for integer vars be rounded according to locks?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/undercover/locksrounding = TRUE

# should we only fix variables in order to obtain a convex problem?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/undercover/onlyconvexify = FALSE

# should the NLP heuristic be called to polish a feasible solution?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/undercover/postnlp = TRUE

# should bounddisjunction constraints be covered (or just copied)?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/undercover/coverbd = FALSE

# should all active cuts from cutpool be copied to constraints in subproblem?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/undercover/copycuts = TRUE

# shall the cover be reused if a conflict was added after an infeasible subproblem?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/undercover/reusecover = FALSE

# priority of heuristic <vbounds>
# [type: int, range: [-536870912,536870911], default: -1106000]
heuristics/vbounds/priority = -1106000

# frequency for calling primal heuristic <vbounds> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/vbounds/freq = -1

# frequency offset for calling primal heuristic <vbounds>
# [type: int, range: [0,2147483647], default: 0]
heuristics/vbounds/freqofs = 0

# maximal depth level to call primal heuristic <vbounds> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/vbounds/maxdepth = -1

# minimum percentage of integer variables that have to be fixable
# [type: real, range: [0,1], default: 0.5]
heuristics/vbounds/minfixingrate = 0.5

# maximum number of nodes to regard in the subproblem
# [type: longint, range: [0,9223372036854775807], default: 5000]
heuristics/vbounds/maxnodes = 5000

# number of nodes added to the contingent of the total nodes
# [type: longint, range: [0,9223372036854775807], default: 500]
heuristics/vbounds/nodesofs = 500

# minimum number of nodes required to start the subproblem
# [type: longint, range: [0,9223372036854775807], default: 500]
heuristics/vbounds/minnodes = 500

# contingent of sub problem nodes in relation to the number of nodes of the original problem
# [type: real, range: [0,1], default: 0.1]
heuristics/vbounds/nodesquot = 0.1

# factor by which vbounds heuristic should at least improve the incumbent
# [type: real, range: [0,1], default: 0.01]
heuristics/vbounds/minimprove = 0.01

# maximum number of propagation rounds during probing (-1 infinity)
# [type: int, range: [-1,536870911], default: 2]
heuristics/vbounds/maxproprounds = 2

# should all active cuts from cutpool be copied to constraints in subproblem?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/vbounds/copycuts = TRUE

# priority of heuristic <veclendiving>
# [type: int, range: [-536870912,536870911], default: -1003100]
heuristics/veclendiving/priority = -1003100

# frequency for calling primal heuristic <veclendiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/veclendiving/freq = 10

# frequency offset for calling primal heuristic <veclendiving>
# [type: int, range: [0,2147483647], default: 4]
heuristics/veclendiving/freqofs = 4

# maximal depth level to call primal heuristic <veclendiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/veclendiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/veclendiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/veclendiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.05]
heuristics/veclendiving/maxlpiterquot = 0.05

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/veclendiving/maxlpiterofs = 1000

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/veclendiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/veclendiving/maxdiveavgquot = 0

# maximal UBQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1], default: 0.1]
heuristics/veclendiving/maxdiveubquotnosol = 0.1

# maximal AVGQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/veclendiving/maxdiveavgquotnosol = 0

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/veclendiving/backtrack = TRUE

# priority of heuristic <zirounding>
# [type: int, range: [-536870912,536870911], default: -500]
heuristics/zirounding/priority = -500

# frequency for calling primal heuristic <zirounding> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 1]
heuristics/zirounding/freq = 1

# frequency offset for calling primal heuristic <zirounding>
# [type: int, range: [0,2147483647], default: 0]
heuristics/zirounding/freqofs = 0

# maximal depth level to call primal heuristic <zirounding> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/zirounding/maxdepth = -1

# determines maximum number of rounding loops
# [type: int, range: [-1,2147483647], default: 2]
heuristics/zirounding/maxroundingloops = 2

# flag to determine if Zirounding is deactivated after a certain percentage of unsuccessful calls
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/zirounding/stopziround = TRUE

# if percentage of found solutions falls below this parameter, Zirounding will be deactivated
# [type: real, range: [0,1], default: 0.02]
heuristics/zirounding/stoppercentage = 0.02

# determines the minimum number of calls before percentage-based deactivation of Zirounding is applied
# [type: int, range: [1,2147483647], default: 1000]
heuristics/zirounding/minstopncalls = 1000

# priority of heuristic <zeroobj>
# [type: int, range: [-536870912,536870911], default: 100]
heuristics/zeroobj/priority = 100

# frequency for calling primal heuristic <zeroobj> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/zeroobj/freq = -1

# frequency offset for calling primal heuristic <zeroobj>
# [type: int, range: [0,2147483647], default: 0]
heuristics/zeroobj/freqofs = 0

# maximal depth level to call primal heuristic <zeroobj> (-1: no limit)
# [type: int, range: [-1,2147483647], default: 0]
heuristics/zeroobj/maxdepth = 0

# maximum number of nodes to regard in the subproblem
# [type: longint, range: [0,9223372036854775807], default: 1000]
heuristics/zeroobj/maxnodes = 1000

# number of nodes added to the contingent of the total nodes
# [type: longint, range: [0,9223372036854775807], default: 100]
heuristics/zeroobj/nodesofs = 100

# minimum number of nodes required to start the subproblem
# [type: longint, range: [0,9223372036854775807], default: 100]
heuristics/zeroobj/minnodes = 100

# maximum number of LP iterations to be performed in the subproblem
# [type: longint, range: [-1,9223372036854775807], default: 5000]
heuristics/zeroobj/maxlpiters = 5000

# contingent of sub problem nodes in relation to the number of nodes of the original problem
# [type: real, range: [0,1], default: 0.1]
heuristics/zeroobj/nodesquot = 0.1

# factor by which zeroobj should at least improve the incumbent
# [type: real, range: [0,1], default: 0.01]
heuristics/zeroobj/minimprove = 0.01

# should all subproblem solutions be added to the original SCIP?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/zeroobj/addallsols = FALSE

# should heuristic only be executed if no primal solution was found, yet?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/zeroobj/onlywithoutsol = TRUE

# priority of heuristic <simplerounding>
# [type: int, range: [-536870912,536870911], default: 0]
heuristics/simplerounding/priority = 0

# frequency for calling primal heuristic <simplerounding> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 1]
heuristics/simplerounding/freq = 1

# frequency offset for calling primal heuristic <simplerounding>
# [type: int, range: [0,2147483647], default: 0]
heuristics/simplerounding/freqofs = 0

# maximal depth level to call primal heuristic <simplerounding> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/simplerounding/maxdepth = -1

# should the heuristic only be called once per node?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/simplerounding/oncepernode = FALSE

# priority of separator <clique>
# [type: int, range: [-536870912,536870911], default: -5000]
separating/clique/priority = -5000

# frequency for calling separator <clique> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
separating/clique/freq = 0

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying separator <clique> (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 0]
separating/clique/maxbounddist = 0

# should separator be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/clique/delay = FALSE

# factor for scaling weights
# [type: real, range: [1,1.79769313486232e+308], default: 1000]
separating/clique/scaleval = 1000

# maximal number of nodes in branch and bound tree (-1: no limit)
# [type: int, range: [-1,2147483647], default: 10000]
separating/clique/maxtreenodes = 10000

# frequency for premature backtracking up to tree level 1 (0: no backtracking)
# [type: int, range: [0,2147483647], default: 1000]
separating/clique/backtrackfreq = 1000

# maximal number of clique cuts separated per separation round (-1: no limit)
# [type: int, range: [-1,2147483647], default: 10]
separating/clique/maxsepacuts = 10

# maximal number of zero-valued variables extending the clique (-1: no limit)
# [type: int, range: [-1,2147483647], default: 1000]
separating/clique/maxzeroextensions = 1000

# maximal memory size of dense clique table (in kb)
# [type: real, range: [0,2097151.99902344], default: 20000]
separating/clique/cliquetablemem = 20000

# minimal density of cliques to use a dense clique table
# [type: real, range: [0,1], default: 0.05]
separating/clique/cliquedensity = 0.05

# priority of separator <cmir>
# [type: int, range: [-536870912,536870911], default: -3000]
separating/cmir/priority = -3000

# frequency for calling separator <cmir> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
separating/cmir/freq = 0

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying separator <cmir> (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 0]
separating/cmir/maxbounddist = 0

# should separator be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/cmir/delay = FALSE

# maximal number of cmir separation rounds per node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 3]
separating/cmir/maxrounds = 3

# maximal number of cmir separation rounds in the root node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 10]
separating/cmir/maxroundsroot = 10

# maximal number of rows to start aggregation with per separation round (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 100]
separating/cmir/maxtries = 100

# maximal number of rows to start aggregation with per separation round in the root node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: -1]
separating/cmir/maxtriesroot = -1

# maximal number of consecutive unsuccessful aggregation tries (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 20]
separating/cmir/maxfails = 20

# maximal number of consecutive unsuccessful aggregation tries in the root node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 100]
separating/cmir/maxfailsroot = 100

# maximal number of aggregations for each row per separation round
# [type: int, range: [0,2147483647], default: 3]
separating/cmir/maxaggrs = 3

# maximal number of aggregations for each row per separation round in the root node
# [type: int, range: [0,2147483647], default: 6]
separating/cmir/maxaggrsroot = 6

# maximal number of cmir cuts separated per separation round
# [type: int, range: [0,2147483647], default: 100]
separating/cmir/maxsepacuts = 100

# maximal number of cmir cuts separated per separation round in the root node
# [type: int, range: [0,2147483647], default: 500]
separating/cmir/maxsepacutsroot = 500

# maximal slack of rows to be used in aggregation
# [type: real, range: [0,1.79769313486232e+308], default: 0]
separating/cmir/maxslack = 0

# maximal slack of rows to be used in aggregation in the root node
# [type: real, range: [0,1.79769313486232e+308], default: 0.1]
separating/cmir/maxslackroot = 0.1

# weight of row density in the aggregation scoring of the rows
# [type: real, range: [0,1.79769313486232e+308], default: 0.0001]
separating/cmir/densityscore = 0.0001

# weight of slack in the aggregation scoring of the rows
# [type: real, range: [0,1.79769313486232e+308], default: 0.001]
separating/cmir/slackscore = 0.001

# maximal density of aggregated row
# [type: real, range: [0,1], default: 0.2]
separating/cmir/maxaggdensity = 0.2

# maximal density of row to be used in aggregation
# [type: real, range: [0,1], default: 0.05]
separating/cmir/maxrowdensity = 0.05

# additional number of variables allowed in row on top of density
# [type: int, range: [0,2147483647], default: 100]
separating/cmir/densityoffset = 100

# maximal row aggregation factor
# [type: real, range: [0,1.79769313486232e+308], default: 10000]
separating/cmir/maxrowfac = 10000

# maximal number of different deltas to try (-1: unlimited)
# [type: int, range: [-1,2147483647], default: -1]
separating/cmir/maxtestdelta = -1

# maximal number of active continuous variables in aggregated row
# [type: int, range: [0,2147483647], default: 10]
separating/cmir/maxconts = 10

# maximal number of active continuous variables in aggregated row in the root node
# [type: int, range: [0,2147483647], default: 10]
separating/cmir/maxcontsroot = 10

# tolerance for bound distances used to select continuous variable in current aggregated constraint to be eliminated
# [type: real, range: [0,1.79769313486232e+308], default: 0.1]
separating/cmir/aggrtol = 0.1

# should negative values also be tested in scaling?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/cmir/trynegscaling = TRUE

# should an additional variable be complemented if f0 = 0?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/cmir/fixintegralrhs = TRUE

# should generated cuts be removed from the LP if they are no longer tight?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/cmir/dynamiccuts = TRUE

# priority of separator <flowcover>
# [type: int, range: [-536870912,536870911], default: -4000]
separating/flowcover/priority = -4000

# frequency for calling separator <flowcover> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
separating/flowcover/freq = 0

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying separator <flowcover> (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 0]
separating/flowcover/maxbounddist = 0

# should separator be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/flowcover/delay = FALSE

# maximal number of separation rounds per node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 5]
separating/flowcover/maxrounds = 5

# maximal number of separation rounds in the root node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 15]
separating/flowcover/maxroundsroot = 15

# maximal number of rows to separate flow cover cuts for per separation round (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 100]
separating/flowcover/maxtries = 100

# maximal number of rows to separate flow cover cuts for per separation round in the root (-1: unlimited)
# [type: int, range: [-1,2147483647], default: -1]
separating/flowcover/maxtriesroot = -1

# maximal number of consecutive fails to generate a cut per separation round (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 50]
separating/flowcover/maxfails = 50

# maximal number of consecutive fails to generate a cut per separation round in the root (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 100]
separating/flowcover/maxfailsroot = 100

# maximal number of flow cover cuts separated per separation round
# [type: int, range: [0,2147483647], default: 100]
separating/flowcover/maxsepacuts = 100

# maximal number of flow cover cuts separated per separation round in the root
# [type: int, range: [0,2147483647], default: 200]
separating/flowcover/maxsepacutsroot = 200

# maximal slack of rows to separate flow cover cuts for
# [type: real, range: [0,1.79769313486232e+308], default: 1.79769313486232e+308]
separating/flowcover/maxslack = 1.79769313486232e+308

# maximal slack of rows to separate flow cover cuts for in the root
# [type: real, range: [0,1.79769313486232e+308], default: 1.79769313486232e+308]
separating/flowcover/maxslackroot = 1.79769313486232e+308

# weight of slack in the scoring of the rows
# [type: real, range: [0,1.79769313486232e+308], default: 0.001]
separating/flowcover/slackscore = 0.001

# maximal density of row to separate flow cover cuts for
# [type: real, range: [0,1], default: 1]
separating/flowcover/maxrowdensity = 1

# should generated cuts be removed from the LP if they are no longer tight?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/flowcover/dynamiccuts = TRUE

# should flow cover cuts be separated for 0-1 single node flow set with reversed arcs in addition?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/flowcover/multbyminusone = TRUE

# cut generation heuristic: maximal number of different deltas to try
# [type: int, range: [0,2147483647], default: 10]
separating/flowcover/maxtestdelta = 10

# priority of separator <gomory>
# [type: int, range: [-536870912,536870911], default: -1000]
separating/gomory/priority = -1000

# frequency for calling separator <gomory> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
separating/gomory/freq = 0

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying separator <gomory> (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 0]
separating/gomory/maxbounddist = 0

# should separator be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/gomory/delay = FALSE

# maximal number of gomory separation rounds per node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 5]
separating/gomory/maxrounds = 5

# maximal number of gomory separation rounds in the root node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 10]
separating/gomory/maxroundsroot = 10

# maximal number of gomory cuts separated per separation round
# [type: int, range: [0,2147483647], default: 50]
separating/gomory/maxsepacuts = 50

# maximal number of gomory cuts separated per separation round in the root node
# [type: int, range: [0,2147483647], default: 200]
separating/gomory/maxsepacutsroot = 200

# maximal rank of a gomory cut that could not be scaled to integral coefficients (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 3]
separating/gomory/maxrank = 3

# maximal rank of a gomory cut that could be scaled to integral coefficients (-1: unlimited)
# [type: int, range: [-1,2147483647], default: -1]
separating/gomory/maxrankintegral = -1

# minimal integrality violation of a basis variable in order to try Gomory cut
# [type: real, range: [0.0001,0.5], default: 0.01]
separating/gomory/away = 0.01

# maximal valid range max(|weights|)/min(|weights|) of row weights
# [type: real, range: [1,1.79769313486232e+308], default: 10000]
separating/gomory/maxweightrange = 10000

# should generated cuts be removed from the LP if they are no longer tight?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/gomory/dynamiccuts = TRUE

# try to scale cuts to integral coefficients
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/gomory/makeintegral = TRUE

# if conversion to integral coefficients failed still consider the cut
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/gomory/forcecuts = TRUE

# separate rows with integral slack
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/gomory/separaterows = TRUE

# should cuts be added to the delayed cut pool?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/gomory/delayedcuts = TRUE

# choose side types of row (lhs/rhs) based on basis information?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/gomory/sidetypebasis = FALSE

# priority of separator <impliedbounds>
# [type: int, range: [-536870912,536870911], default: -50]
separating/impliedbounds/priority = -50

# frequency for calling separator <impliedbounds> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
separating/impliedbounds/freq = 0

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying separator <impliedbounds> (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 0]
separating/impliedbounds/maxbounddist = 0

# should separator be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/impliedbounds/delay = FALSE

# priority of separator <intobj>
# [type: int, range: [-536870912,536870911], default: -100]
separating/intobj/priority = -100

# frequency for calling separator <intobj> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: -1]
separating/intobj/freq = -1

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying separator <intobj> (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 0]
separating/intobj/maxbounddist = 0

# should separator be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/intobj/delay = FALSE

# priority of separator <mcf>
# [type: int, range: [-536870912,536870911], default: -10000]
separating/mcf/priority = -10000

# frequency for calling separator <mcf> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
separating/mcf/freq = 0

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying separator <mcf> (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 0]
separating/mcf/maxbounddist = 0

# should separator be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/mcf/delay = FALSE

# number of clusters to generate in the shrunken network -- default separation
# [type: int, range: [2,32], default: 5]
separating/mcf/nclusters = 5

# maximal valid range max(|weights|)/min(|weights|) of row weights
# [type: real, range: [1,1.79769313486232e+308], default: 1000000]
separating/mcf/maxweightrange = 1000000

# maximal number of different deltas to try (-1: unlimited)  -- default separation
# [type: int, range: [-1,2147483647], default: 20]
separating/mcf/maxtestdelta = 20

# should negative values also be tested in scaling?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/mcf/trynegscaling = FALSE

# should an additional variable be complemented if f0 = 0?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/mcf/fixintegralrhs = TRUE

# should generated cuts be removed from the LP if they are no longer tight?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/mcf/dynamiccuts = TRUE

# model type of network (0: auto, 1:directed, 2:undirected)
# [type: int, range: [0,2], default: 0]
separating/mcf/modeltype = 0

# maximal number of mcf cuts separated per separation round
# [type: int, range: [-1,2147483647], default: 100]
separating/mcf/maxsepacuts = 100

# maximal number of mcf cuts separated per separation round in the root node  -- default separation
# [type: int, range: [-1,2147483647], default: 200]
separating/mcf/maxsepacutsroot = 200

# maximum inconsistency ratio for separation at all
# [type: real, range: [0,1.79769313486232e+308], default: 0.02]
separating/mcf/maxinconsistencyratio = 0.02

# maximum inconsistency ratio of arcs not to be deleted
# [type: real, range: [0,1.79769313486232e+308], default: 0.5]
separating/mcf/maxarcinconsistencyratio = 0.5

# should we separate only if the cuts shores are connected?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/mcf/checkcutshoreconnectivity = TRUE

# should we separate inequalities based on single-node cuts?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/mcf/separatesinglenodecuts = TRUE

# should we separate flowcutset inequalities on the network cuts?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/mcf/separateflowcutset = TRUE

# should we separate knapsack cover inequalities on the network cuts?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/mcf/separateknapsack = TRUE

# priority of separator <oddcycle>
# [type: int, range: [-536870912,536870911], default: -15000]
separating/oddcycle/priority = -15000

# frequency for calling separator <oddcycle> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: -1]
separating/oddcycle/freq = -1

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying separator <oddcycle> (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 1]
separating/oddcycle/maxbounddist = 1

# should separator be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/oddcycle/delay = FALSE

# should the search method by Groetschel, Lovasz, Schrijver be used? Otherwise use levelgraph method by Hoffman, Padberg.
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/oddcycle/usegls = TRUE

# should odd cycle cuts be lifted?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/oddcycle/liftoddcycles = FALSE

# maximal number of oddcycle cuts separated per separation round
# [type: int, range: [0,2147483647], default: 5000]
separating/oddcycle/maxsepacuts = 5000

# maximal number of oddcycle cuts separated per separation round in the root node
# [type: int, range: [0,2147483647], default: 5000]
separating/oddcycle/maxsepacutsroot = 5000

# maximal number of oddcycle separation rounds per node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 10]
separating/oddcycle/maxrounds = 10

# maximal number of oddcycle separation rounds in the root node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 10]
separating/oddcycle/maxroundsroot = 10

# factor for scaling of the arc-weights
# [type: int, range: [1,2147483647], default: 1000]
separating/oddcycle/scalingfactor = 1000

# add links between a variable and its negated
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/oddcycle/addselfarcs = TRUE

# try to repair violated cycles with double appearance of a variable
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/oddcycle/repaircycles = TRUE

# separate triangles found as 3-cycles or repaired larger cycles
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/oddcycle/includetriangles = TRUE

# even if a variable is already covered by a cut, still try it as start node for a cycle search
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/oddcycle/multiplecuts = FALSE

# even if a variable is already covered by a cut, still allow another cut to cover it too
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/oddcycle/allowmultiplecuts = TRUE

# choose lifting candidate by coef*lpvalue or only by coef
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/oddcycle/lpliftcoef = FALSE

# calculate lifting coefficient of every candidate in every step (or only if its chosen)
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/oddcycle/recalcliftcoef = TRUE

# use sorted variable array (unsorted(0),maxlp(1),minlp(2),maxfrac(3),minfrac(4))
# [type: int, range: [0,4], default: 3]
separating/oddcycle/sortswitch = 3

# sort level of the root neighbors by fractionality (maxfrac)
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/oddcycle/sortrootneighbors = TRUE

# percentage of variables to try the chosen method on [0-100]
# [type: int, range: [0,100], default: 0]
separating/oddcycle/percenttestvars = 0

# offset of variables to try the chosen method on (additional to the percentage of testvars)
# [type: int, range: [0,2147483647], default: 100]
separating/oddcycle/offsettestvars = 100

# percentage of nodes allowed in the same level of the level graph [0-100]
# [type: int, range: [0,100], default: 100]
separating/oddcycle/maxpernodeslevel = 100

# offset of nodes allowed in the same level of the level graph (additional to the percentage of levelnodes)
# [type: int, range: [0,2147483647], default: 10]
separating/oddcycle/offsetnodeslevel = 10

# maximal number of levels in level graph
# [type: int, range: [0,2147483647], default: 20]
separating/oddcycle/maxnlevels = 20

# maximal number of oddcycle cuts generated per chosen variable as root of the level graph
# [type: int, range: [0,2147483647], default: 1]
separating/oddcycle/maxcutsroot = 1

# maximal number of oddcycle cuts generated in every level of the level graph
# [type: int, range: [0,2147483647], default: 50]
separating/oddcycle/maxcutslevel = 50

# minimal weight on an edge (in level graph or bipartite graph)
# [type: int, range: [0,2147483647], default: 0]
separating/oddcycle/maxreference = 0

# number of unsuccessful calls at current node
# [type: int, range: [0,2147483647], default: 3]
separating/oddcycle/maxunsucessfull = 3

# maximal number of other cuts s.t. separation is applied (-1 for direct call)
# [type: int, range: [-1,2147483647], default: -1]
separating/oddcycle/cutthreshold = -1

# priority of separator <strongcg>
# [type: int, range: [-536870912,536870911], default: -2000]
separating/strongcg/priority = -2000

# frequency for calling separator <strongcg> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 0]
separating/strongcg/freq = 0

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying separator <strongcg> (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 0]
separating/strongcg/maxbounddist = 0

# should separator be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/strongcg/delay = FALSE

# maximal number of strong CG separation rounds per node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 5]
separating/strongcg/maxrounds = 5

# maximal number of strong CG separation rounds in the root node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 20]
separating/strongcg/maxroundsroot = 20

# maximal number of strong CG cuts separated per separation round
# [type: int, range: [0,2147483647], default: 50]
separating/strongcg/maxsepacuts = 50

# maximal number of strong CG cuts separated per separation round in the root node
# [type: int, range: [0,2147483647], default: 500]
separating/strongcg/maxsepacutsroot = 500

# maximal valid range max(|weights|)/min(|weights|) of row weights
# [type: real, range: [1,1.79769313486232e+308], default: 10000]
separating/strongcg/maxweightrange = 10000

# should generated cuts be removed from the LP if they are no longer tight?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/strongcg/dynamiccuts = TRUE

# priority of separator <zerohalf>
# [type: int, range: [-536870912,536870911], default: -6000]
separating/zerohalf/priority = -6000

# frequency for calling separator <zerohalf> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: -1]
separating/zerohalf/freq = -1

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying separator <zerohalf> (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 0]
separating/zerohalf/maxbounddist = 0

# should separator be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/zerohalf/delay = FALSE

# maximal number of zerohalf separation rounds per node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 5]
separating/zerohalf/maxrounds = 5

# maximal number of zerohalf separation rounds in the root node (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 10]
separating/zerohalf/maxroundsroot = 10

# maximal number of {0,1/2}-cuts separated per separation round
# [type: int, range: [0,2147483647], default: 50]
separating/zerohalf/maxsepacuts = 50

# maximal number of {0,1/2}-cuts separated per separation round in the root node
# [type: int, range: [0,2147483647], default: 500]
separating/zerohalf/maxsepacutsroot = 500

# should generated cuts be removed from the LP if they are no longer tight?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/zerohalf/dynamiccuts = TRUE

# maximal number of {0,1/2}-cuts determined per separation round
   #                      (this includes separated but inefficacious cuts)
# [type: int, range: [0,1000000], default: 100]
separating/zerohalf/maxcutsfound = 100

# maximal number of {0,1/2}-cuts determined per separation round in the root node
   #                      (this includes separated but inefficacious cuts)
# [type: int, range: [0,1000000], default: 1000]
separating/zerohalf/maxcutsfoundroot = 1000

# separating cuts only if depth <= maxdepth (-1: unlimited)
# [type: int, range: [-1,2147483647], default: -1]
separating/zerohalf/maxdepth = -1

# maximal number of calls (-1: unlimited)
# [type: longint, range: [-1,9223372036854775807], default: -1]
separating/zerohalf/maxncalls = -1

# should continuous variables be relaxed by adding variable bounds?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/zerohalf/relaxcontvars = FALSE

# should rows be scaled to make fractional coefficients integer?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/zerohalf/scalefraccoeffs = TRUE

# should zerohalf cuts found in previous callbacks ignored?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/zerohalf/ignoreprevzhcuts = FALSE

# should only original LP rows be considered (i.e. ignore previously added LP rows)?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/zerohalf/onlyorigrows = FALSE

# should zerohalf cuts be filtered using a cutpool?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/zerohalf/usezhcutpool = TRUE

# should cuts be added to the delayed cut pool?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/zerohalf/delayedcuts = TRUE

# maximal number of different deltas to try for cmir (-1: unlimited, 0: delta=1)
# [type: int, range: [-1,2147483647], default: 10]
separating/zerohalf/maxtestdelta = 10

# should negative values also be tested in scaling for cmir?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/zerohalf/trynegscaling = TRUE

# should problem be decomposed into subproblems (if possible) before applying preprocessing?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/zerohalf/preprocessing/decomposeproblem = FALSE

# value of delta parameter used in preprocessing method 'd'
# [type: real, range: [0,1], default: 0.5]
separating/zerohalf/preprocessing/delta = 0.5

# preprocessing methods and ordering:
   #                      'd' columns with small LP solution,
   #                      'G' modified Gaussian elimination,
   #                      'i' identical columns,
   #                      'I' identical rows,
   #                      'L' large slack rows,
   #                      'M' large slack rows (minslack),
   #                      's' column singletons,
   #                      'X' add trivial zerohalf cuts,
   #                      'z' zero columns,
   #                      'Z' zero rows,
   #                      'C' fast {'z','s'},
   #                      'R' fast {'Z','L','I'}
   #
   #                      '-' no preprocessing
   #
# [type: string, default: "CXGXIM"]
separating/zerohalf/preprocessing/ppmethods = "CXGXIM"

# should the cuts be forced to enter the LP?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/zerohalf/separating/forcecutstolp = FALSE

# should the cuts be forced to enter SCIP's sepastore?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
separating/zerohalf/separating/forcecutstosepastore = FALSE

# minimal violation of a {0,1/2}-cut to be separated
# [type: real, range: [0.001,0.5], default: 0.3]
separating/zerohalf/separating/minviolation = 0.3

# separating methods and ordering:
   #                      '!' stop further processing if a cut was found,
   #                      '2' exact polynomial time algorithm (only if matrix has max 2 odd entries per row),
   #                      'e' enumeration heuristics (k=1: try all preprocessed rows),
   #                      'E' enumeration heuristics (k=2: try all combinations of up to two preprocessed rows),
   #                      'g' Extended Gaussian elimination heuristics,
   #                      's' auxiliary IP heuristics (i.e. number of solved nodes is limited)
   #                      'S' auxiliary IP exact      (i.e. unlimited number of nodes)
   #
   #                      '-' no processing
   #
# [type: string, default: "2g"]
separating/zerohalf/separating/sepamethods = "2g"

# optional settings file of the auxiliary IP (-: none)
# [type: string, default: "-"]
separating/zerohalf/separating/auxip/settingsfile = "-"

# limits/solutions setting of the auxiliary IP
# [type: int, range: [-1,2147483647], default: -1]
separating/zerohalf/separating/auxip/sollimit = -1

# penalty factor used with objective function 'p' of auxiliary IP
# [type: real, range: [0,1], default: 0.001]
separating/zerohalf/separating/auxip/penaltyfactor = 0.001

# should all (proper) solutions of the auxiliary IP be used to generate cuts instead of using only the best?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
separating/zerohalf/separating/auxip/useallsols = TRUE

# auxiliary IP objective:
   #                      'v' maximize cut violation,
   #                      'u' minimize number of aggregated rows in cut,
   #                      'w' minimize number of aggregated rows in cut
   #                          weighted by the number of rows in the aggregation,
   #                      'p' maximize cut violation and penalize a high number
   #                          of aggregated rows in the cut weighted by the number
   #                          of rows in the aggregation and the penalty factor p
   #
# [type: char, range: {uvwp}, default: v]
separating/zerohalf/separating/auxip/objective = v

# priority of relaxation handler <gcg>
# [type: int, range: [-536870912,536870911], default: -1]
relaxing/gcg/priority = -1

# frequency for calling relaxation handler <gcg> (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: 1]
relaxing/gcg/freq = 1

# maximal number of variables created in one farkas pricing round
# [type: int, range: [1,2147483647], default: 10]
pricing/masterpricer/maxvarsroundfarkas = 10

# part of the submips that are solved before Farkas pricing round is aborted, if variables have been found yet? (1.0 = solve all pricing MIPs)
# [type: real, range: [0,1], default: 1]
pricing/masterpricer/mipsrelfarkas = 1

# maximal number of pricing mips leading to new variables solved solved in one redcost pricing round
# [type: int, range: [1,2147483647], default: 2147483647]
pricing/masterpricer/maxsuccessfulmipsredcost = 2147483647

# maximal number of variables created in one redcost pricing round
# [type: int, range: [0,2147483647], default: 100]
pricing/masterpricer/maxvarsroundredcost = 100

# maximal number of variables created in one redcost pricing round at the root node
# [type: int, range: [0,2147483647], default: 100]
pricing/masterpricer/maxvarsroundredcostroot = 100

# maximal number of pricing rounds per node after the root node
# [type: int, range: [0,2147483647], default: 2147483647]
pricing/masterpricer/maxroundsredcost = 2147483647

# part of the submips that are solved before redcost pricing round is aborted at the root node, if variables have been found yed? (1.0 = solve all pricing MIPs)
# [type: real, range: [0,1], default: 1]
pricing/masterpricer/mipsrelredcostroot = 1

# part of the submips that are solved before redcost pricing round is aborted, if variables have been found yed? (1.0 = solve all pricing MIPs)
# [type: real, range: [0,1], default: 1]
pricing/masterpricer/mipsrelredcost = 1

# maximal number of variables added for each block in a pricinground
# [type: int, range: [0,2147483647], default: 2147483647]
pricing/masterpricer/maxsolsprob = 2147483647

# should pricing be performed heuristically before solving the MIPs to optimality?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
pricing/masterpricer/useheurpricing = FALSE

# should pricing be aborted due to integral objective function?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
pricing/masterpricer/abortpricingint = TRUE

# should pricing be aborted due to small gap between dual bound and RMP objective?
# [type: real, range: [0,1], default: 0]
pricing/masterpricer/abortpricinggap = 0

# part of the submips that are solved and lead to new variables before pricing round is aborted? (1.0 = solve all pricing MIPs)
# [type: real, range: [0,1], default: 1]
pricing/masterpricer/successfulsubmipsrel = 1

# should additional informations concerning the pricing process be displayed?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
pricing/masterpricer/dispinfos = FALSE

# which sorting method should be used to sort the pricing problems (0 = order of pricing problems, 1 = according to dual solution of convexity constraint, 2 = according to reliability from previous round)
# [type: int, range: [0,5], default: 2]
pricing/masterpricer/sorting = 2

# how many threads should be used to concurrently solve the pricing problem (0 to guess threads by OpenMP)
# [type: int, range: [0,4096], default: 0]
pricing/masterpricer/threads = 0

# should stabilization be performed?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
pricing/masterpricer/stabilization = TRUE

# should the cutoffbound be applied in master LP solving (0: on, 1:off, 2:auto)?
# [type: int, range: [0,2], default: 2]
pricing/masterpricer/disablecutoff = 2

# frequency at which all pricingproblems should be solved (0 to disable)
# [type: int, range: [0,2147483647], default: 10]
pricing/masterpricer/eagerfreq = 10

# should bounds on variables be enforced by constraints(TRUE) or by bounds(FALSE)
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
branching/orig/enforcebycons = FALSE

# should branching be performed on the most fractional variable instead of the first variable?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
branching/orig/mostfrac = FALSE

# should pseudocosts be used to determine the variable on which the branching is performed?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
branching/orig/usepseudocosts = TRUE

# should strong branching with propagation be used to determine the variable on which the branching is performed?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
branching/orig/usepsstrong = FALSE

# weight in score calculations for conflict score
# [type: real, range: [-1.79769313486232e+308,1.79769313486232e+308], default: 0.01]
branching/relpsprob/conflictweight = 0.01

# weight in score calculations for conflict length score
# [type: real, range: [-1.79769313486232e+308,1.79769313486232e+308], default: 0.0001]
branching/relpsprob/conflictlengthweight = 0.0001

# weight in score calculations for inference score
# [type: real, range: [-1.79769313486232e+308,1.79769313486232e+308], default: 0.1]
branching/relpsprob/inferenceweight = 0.1

# weight in score calculations for cutoff score
# [type: real, range: [-1.79769313486232e+308,1.79769313486232e+308], default: 0.0001]
branching/relpsprob/cutoffweight = 0.0001

# weight in score calculations for pseudo cost score
# [type: real, range: [-1.79769313486232e+308,1.79769313486232e+308], default: 1]
branching/relpsprob/pscostweight = 1

# minimal value for minimum pseudo cost size to regard pseudo cost value as reliable
# [type: real, range: [0,1.79769313486232e+308], default: 1]
branching/relpsprob/minreliable = 1

# maximal value for minimum pseudo cost size to regard pseudo cost value as reliable
# [type: real, range: [0,1.79769313486232e+308], default: 8]
branching/relpsprob/maxreliable = 8

# maximal fraction of branching LP iterations compared to node relaxation LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.5]
branching/relpsprob/iterquot = 0.5

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 100000]
branching/relpsprob/iterofs = 100000

# maximal number of further variables evaluated without better score
# [type: int, range: [1,2147483647], default: 8]
branching/relpsprob/maxlookahead = 8

# maximal number of candidates initialized with strong branching per node
# [type: int, range: [0,2147483647], default: 100]
branching/relpsprob/initcand = 100

# maximal number of bound tightenings before the node is immediately reevaluated (-1: unlimited)
# [type: int, range: [-1,2147483647], default: 20]
branching/relpsprob/maxbdchgs = 20

# minimal number of bound tightenings before bound changes are applied
# [type: int, range: [1,2147483647], default: 1]
branching/relpsprob/minbdchgs = 1

# shall the LP be solved during probing? (TRUE)
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
branching/relpsprob/uselp = TRUE

# reliability value for probing
# [type: real, range: [0,1], default: 0.8]
branching/relpsprob/reliability = 0.8

# flag to indicate whether the solver is enabled
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
pricingsolver/knapsack/enabled = TRUE

# flag to indicate whether the solver is enabled
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
pricingsolver/mip/enabled = TRUE

# should solutions of the pricing MIPs be checked for duplicity?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
pricingsolver/mip/checksols = TRUE

# settings file for pricing problems
# [type: string, default: "-"]
pricingsolver/mip/settingsfile = "-"

# should propagated bound changes in the original be enforced in the master (only proper vars)?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
relaxing/gcg/enforceproper = TRUE

# filename to write all bounds to
# [type: string, default: ""]
eventhdlr/solvingstats/filename = ""

# should discretization (TRUE) or convexification (FALSE) approach be used?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
relaxing/gcg/discretization = TRUE

# should identical blocks be aggregated (only for discretization approach)?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
relaxing/gcg/aggregation = TRUE

# should additional information about the blocks be displayed?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
relaxing/gcg/dispinfos = FALSE

# priority of branching rule <empty>
# [type: int, range: [-536870912,536870911], default: 1000000]
branching/empty/priority = 1000000

# maximal depth level, up to which branching rule <empty> should be used (-1 for no limit)
# [type: int, range: [-1,2147483647], default: -1]
branching/empty/maxdepth = -1

# maximal relative distance from current node's dual bound to primal bound compared to best node's dual bound for applying branching rule (0.0: only on current best node, 1.0: on all nodes)
# [type: real, range: [0,1], default: 1]
branching/empty/maxbounddist = 1

# frequency for separating cuts (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: -1]
constraints/origbranch/sepafreq = -1

# frequency for propagating domains (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: -1]
constraints/origbranch/propfreq = -1

# timing when constraint propagation should be called (1:BEFORELP, 2:DURINGLPLOOP, 4:AFTERLPLOOP, 15:ALWAYS)
# [type: int, range: [1,15], default: 15]
constraints/origbranch/timingmask = 15

# frequency for using all instead of only the useful constraints in separation, propagation and enforcement (-1: never, 0: only in first evaluation)
# [type: int, range: [-1,2147483647], default: 100]
constraints/origbranch/eagerfreq = 100

# maximal number of presolving rounds the constraint handler participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
constraints/origbranch/maxprerounds = -1

# should separation method be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/origbranch/delaysepa = FALSE

# should propagation method be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/origbranch/delayprop = FALSE

# should presolving method be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/origbranch/delaypresol = FALSE

# frequency for separating cuts (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: -1]
constraints/decomp/sepafreq = -1

# frequency for propagating domains (-1: never, 0: only in root node)
# [type: int, range: [-1,2147483647], default: -1]
constraints/decomp/propfreq = -1

# timing when constraint propagation should be called (1:BEFORELP, 2:DURINGLPLOOP, 4:AFTERLPLOOP, 15:ALWAYS)
# [type: int, range: [1,15], default: 8]
constraints/decomp/timingmask = 8

# frequency for using all instead of only the useful constraints in separation, propagation and enforcement (-1: never, 0: only in first evaluation)
# [type: int, range: [-1,2147483647], default: -1]
constraints/decomp/eagerfreq = -1

# maximal number of presolving rounds the constraint handler participates in (-1: no limit)
# [type: int, range: [-1,2147483647], default: 0]
constraints/decomp/maxprerounds = 0

# should separation method be delayed, if other separators found cuts?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/decomp/delaysepa = TRUE

# should propagation method be delayed, if other propagators found reductions?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/decomp/delayprop = TRUE

# should presolving method be delayed, if other presolvers found reductions?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
constraints/decomp/delaypresol = TRUE

# indicates whether to create a decomposition with all constraints in the master if no other specified
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
constraints/decomp/createbasicdecomp = FALSE

# flag to indicate whether detector <connected> is enabled
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
detectors/connected/enabled = TRUE

# flag to indicate whether detector <connected> should be skipped if others found decompositions
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
detectors/connected/skip = FALSE

# priority of detector <connected>
# [type: int, range: [-2147483648,2147483647], default: 0]
detectors/connected/priority = 0

# Controls whether SETPPC constraints chould be ignored while detecting and be directly placed in the master
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
detectors/connected/setppcinmaster = TRUE

# flag to indicate whether detector <arrowheur> is enabled
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
detectors/arrowheur/enabled = FALSE

# flag to indicate whether detector <arrowheur> should be skipped if others found decompositions
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
detectors/arrowheur/skip = FALSE

# priority of detector <arrowheur>
# [type: int, range: [-2147483648,2147483647], default: 1000]
detectors/arrowheur/priority = 1000

# The maximal number of blocks
# [type: int, range: [2,1000000], default: 20]
detectors/arrowheur/maxblocks = 20

# The minimal number of blocks
# [type: int, range: [2,1000000], default: 2]
detectors/arrowheur/minblocks = 2

# factor on how heavy equality (beta) and inequality constraints are measured
# [type: real, range: [0,1], default: 0.5]
detectors/arrowheur/beta = 0.5

# factor on how heavy the standard deviation of the coefficients is measured
# [type: real, range: [0,1e+20], default: 0]
detectors/arrowheur/alpha = 0

# Weight of a variable hyperedge
# [type: int, range: [0,1000000], default: 1]
detectors/arrowheur/varWeight = 1

# Weight of a binary variable hyperedge
# [type: int, range: [0,1000000], default: 2]
detectors/arrowheur/varWeightBinary = 2

# Weight of a continuos variable hyperedge
# [type: int, range: [0,1000000], default: 1]
detectors/arrowheur/varWeightContinous = 1

# Weight of a implicit integer variable hyperedge
# [type: int, range: [0,1000000], default: 2]
detectors/arrowheur/varWeightImplint = 2

# Weight of a integer variable hyperedge
# [type: int, range: [0,1000000], default: 2]
detectors/arrowheur/varWeightInteger = 2

# Weight of a constraint hyperedge
# [type: int, range: [0,1000000], default: 5]
detectors/arrowheur/consWeight = 5

# Whether to clean up temporary files
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
detectors/arrowheur/tidy = TRUE

# random seed for hmetis
# [type: int, range: [-1,2147483647], default: 1]
detectors/arrowheur/randomseed = 1

# percentage of dummy nodes for metis
# [type: real, range: [0,1], default: 0.2]
detectors/arrowheur/dummynodes = 0.2

# Weight for constraint hyperedges that are setpartitioning or covering constraints
# [type: int, range: [0,1000000], default: 5]
detectors/arrowheur/consWeightSetppc = 5

# Unbalance factor for metis
# [type: real, range: [0,1e+20], default: 5]
detectors/arrowheur/ubfactor = 5

# Should the metis output be displayed
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
detectors/arrowheur/metisverbose = FALSE

# Should the rb or kway method be used for partitioning by metis
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
detectors/arrowheur/metisuseptyperb = TRUE

# Should the problem be used for metis files or a temporary name
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
detectors/arrowheur/realname = FALSE

# Type of the graph: 'c' column hypergraph, 'r' row hypergraph, 'a' column-row hypergraph
# [type: char, range: {cra}, default: r]
detectors/arrowheur/type = r

# flag to indicate whether detector <stairheur> is enabled
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
detectors/stairheur/enabled = FALSE

# flag to indicate whether detector <stairheur> should be skipped if others found decompositions
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
detectors/stairheur/skip = FALSE

# priority of detector <stairheur>
# [type: int, range: [-2147483648,2147483647], default: 1200]
detectors/stairheur/priority = 1200

# The number of constraints per block (static blocking only)
# [type: int, range: [2,1000000], default: 32]
detectors/stairheur/nconssperblock = 32

# The maximal number of blocks
# [type: int, range: [2,1000000], default: 20]
detectors/stairheur/maxblocks = 20

# The minimal number of blocks
# [type: int, range: [2,1000000], default: 2]
detectors/stairheur/minblocks = 2

# The desired number of blocks. 0 means automatic determination of the number of blocks.
# [type: int, range: [0,1000000], default: 0]
detectors/stairheur/desiredblocks = 0

# Enable blocking type 'dynamic'
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
detectors/stairheur/dynamicblocking = FALSE

# Enable blocking type 'static'
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
detectors/stairheur/staticblocking = TRUE

# Enable blocking type 'as soon as possible
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
detectors/stairheur/blockingassoonaspossible = FALSE

# Enables multiple decompositions for all enabled blocking types. Ranging from minblocks to maxblocks
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
detectors/stairheur/multipledecomps = TRUE

# The maximum number of iterations of the ROC-algorithm. -1 for no limit
# [type: int, range: [-1,1000000], default: 1000000]
detectors/stairheur/maxiterationsROC = 1000000

# flag to indicate whether detector <staircase> is enabled
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
detectors/staircase/enabled = TRUE

# flag to indicate whether detector <staircase> should be skipped if others found decompositions
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
detectors/staircase/skip = FALSE

# priority of detector <staircase>
# [type: int, range: [-2147483648,2147483647], default: 200]
detectors/staircase/priority = 200

# flag to indicate whether detector <random> is enabled
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
detectors/random/enabled = FALSE

# flag to indicate whether detector <random> should be skipped if others found decompositions
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
detectors/random/skip = FALSE

# priority of detector <random>
# [type: int, range: [-2147483648,2147483647], default: -10]
detectors/random/priority = -10

# random seed for the random number generator, -1 is the current time
# [type: int, range: [-1,2147483647], default: -1]
detectors/random/seed = -1

# the maximal number of blocks, -1 defaults to avgconsperblock
# [type: int, range: [-1,2147483647], default: -1]
detectors/random/maxblocks = -1

# average constraints per block
# [type: int, range: [1,10000], default: 100]
detectors/random/avgconsperblock = 100

# flag to indicate whether detector <colors> is enabled
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
detectors/colors/enabled = FALSE

# flag to indicate whether detector <colors> should be skipped if others found decompositions
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
detectors/colors/skip = FALSE

# priority of detector <colors>
# [type: int, range: [-2147483648,2147483647], default: 0]
detectors/colors/priority = 0

# flag to indicate whether detector <cutpacking> is enabled
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
detectors/cutpacking/enabled = FALSE

# flag to indicate whether detector <cutpacking> should be skipped if others found decompositions
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
detectors/cutpacking/skip = FALSE

# priority of detector <cutpacking>
# [type: int, range: [-2147483648,2147483647], default: 1100]
detectors/cutpacking/priority = 1100

# should the stoer-wagner algorithm or metis be used for finding a minimal cut
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
staircase/algorithm = TRUE

# Should the blocks consist of a certain number of constraints
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
staircase/fixedblocks = FALSE

# number of constraints per block
# [type: int, range: [1,2147483647], default: 200]
staircase/blocksize = 200

# Whether to clean up temporary files
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
staircase/tidy = TRUE

# random seed for hmetis
# [type: int, range: [-1,2147483647], default: 1]
staircase/randomseed = 1

# Unbalance factor for metis
# [type: real, range: [0,1e+20], default: 5]
staircase/ubfactor = 5

# Should the metis output be displayed
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
staircase/metisverbose = FALSE

# Should the rb or kway method be used for partitioning by metis
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
staircase/metisuseptyperb = TRUE

# flag to indicate whether detector <isomorph> is enabled
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
detectors/isomorph/enabled = TRUE

# flag to indicate whether detector <isomorph> should be skipped if others found decompositions
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
detectors/isomorph/skip = TRUE

# priority of detector <isomorph>
# [type: int, range: [-2147483648,2147483647], default: 100]
detectors/isomorph/priority = 100

# priority of heuristic <gcgcoefdiving>
# [type: int, range: [-536870912,536870911], default: -1001000]
heuristics/gcgcoefdiving/priority = -1001000

# frequency for calling primal heuristic <gcgcoefdiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/gcgcoefdiving/freq = 10

# frequency offset for calling primal heuristic <gcgcoefdiving>
# [type: int, range: [0,2147483647], default: 1]
heuristics/gcgcoefdiving/freqofs = 1

# maximal depth level to call primal heuristic <gcgcoefdiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/gcgcoefdiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/gcgcoefdiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/gcgcoefdiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.05]
heuristics/gcgcoefdiving/maxlpiterquot = 0.05

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/gcgcoefdiving/maxlpiterofs = 1000

# maximal number of allowed pricing rounds (-1: no limit)
# [type: int, range: [-1,2147483647], default: 0]
heuristics/gcgcoefdiving/maxpricerounds = 0

# perform pricing only if infeasibility is encountered
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/gcgcoefdiving/usefarkasonly = TRUE

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/gcgcoefdiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/gcgcoefdiving/maxdiveavgquot = 0

# maximal UBQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1], default: 0.1]
heuristics/gcgcoefdiving/maxdiveubquotnosol = 0.1

# maximal AVGQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/gcgcoefdiving/maxdiveavgquotnosol = 0

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/gcgcoefdiving/backtrack = TRUE

# calculate the number of locks w.r.t. the master LP?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/gcgcoefdiving/usemasterlocks = FALSE

# priority of heuristic <gcgfracdiving>
# [type: int, range: [-536870912,536870911], default: -1003000]
heuristics/gcgfracdiving/priority = -1003000

# frequency for calling primal heuristic <gcgfracdiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/gcgfracdiving/freq = 10

# frequency offset for calling primal heuristic <gcgfracdiving>
# [type: int, range: [0,2147483647], default: 3]
heuristics/gcgfracdiving/freqofs = 3

# maximal depth level to call primal heuristic <gcgfracdiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/gcgfracdiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/gcgfracdiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/gcgfracdiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.05]
heuristics/gcgfracdiving/maxlpiterquot = 0.05

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/gcgfracdiving/maxlpiterofs = 1000

# maximal number of allowed pricing rounds (-1: no limit)
# [type: int, range: [-1,2147483647], default: 0]
heuristics/gcgfracdiving/maxpricerounds = 0

# perform pricing only if infeasibility is encountered
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/gcgfracdiving/usefarkasonly = TRUE

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/gcgfracdiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/gcgfracdiving/maxdiveavgquot = 0

# maximal UBQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1], default: 0.1]
heuristics/gcgfracdiving/maxdiveubquotnosol = 0.1

# maximal AVGQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/gcgfracdiving/maxdiveavgquotnosol = 0

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/gcgfracdiving/backtrack = TRUE

# calculate the fractionalities w.r.t. the master LP?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/gcgfracdiving/usemasterfracs = FALSE

# priority of heuristic <gcgguideddiving>
# [type: int, range: [-536870912,536870911], default: -1007000]
heuristics/gcgguideddiving/priority = -1007000

# frequency for calling primal heuristic <gcgguideddiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/gcgguideddiving/freq = 10

# frequency offset for calling primal heuristic <gcgguideddiving>
# [type: int, range: [0,2147483647], default: 7]
heuristics/gcgguideddiving/freqofs = 7

# maximal depth level to call primal heuristic <gcgguideddiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/gcgguideddiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/gcgguideddiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/gcgguideddiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.05]
heuristics/gcgguideddiving/maxlpiterquot = 0.05

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/gcgguideddiving/maxlpiterofs = 1000

# maximal number of allowed pricing rounds (-1: no limit)
# [type: int, range: [-1,2147483647], default: 0]
heuristics/gcgguideddiving/maxpricerounds = 0

# perform pricing only if infeasibility is encountered
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/gcgguideddiving/usefarkasonly = TRUE

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/gcgguideddiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/gcgguideddiving/maxdiveavgquot = 0

# maximal UBQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1], default: 0.1]
heuristics/gcgguideddiving/maxdiveubquotnosol = 0.1

# maximal AVGQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/gcgguideddiving/maxdiveavgquotnosol = 0

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/gcgguideddiving/backtrack = TRUE

# calculate the fractionalities w.r.t. the master LP?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/gcgguideddiving/usemasterfracs = FALSE

# priority of heuristic <gcglinesdiving>
# [type: int, range: [-536870912,536870911], default: -1006000]
heuristics/gcglinesdiving/priority = -1006000

# frequency for calling primal heuristic <gcglinesdiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/gcglinesdiving/freq = 10

# frequency offset for calling primal heuristic <gcglinesdiving>
# [type: int, range: [0,2147483647], default: 6]
heuristics/gcglinesdiving/freqofs = 6

# maximal depth level to call primal heuristic <gcglinesdiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/gcglinesdiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/gcglinesdiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/gcglinesdiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.05]
heuristics/gcglinesdiving/maxlpiterquot = 0.05

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/gcglinesdiving/maxlpiterofs = 1000

# maximal number of allowed pricing rounds (-1: no limit)
# [type: int, range: [-1,2147483647], default: 0]
heuristics/gcglinesdiving/maxpricerounds = 0

# perform pricing only if infeasibility is encountered
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/gcglinesdiving/usefarkasonly = TRUE

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/gcglinesdiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/gcglinesdiving/maxdiveavgquot = 0

# maximal UBQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1], default: 0.1]
heuristics/gcglinesdiving/maxdiveubquotnosol = 0.1

# maximal AVGQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/gcglinesdiving/maxdiveavgquotnosol = 0

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/gcglinesdiving/backtrack = TRUE

# priority of heuristic <gcgpscostdiving>
# [type: int, range: [-536870912,536870911], default: -1002000]
heuristics/gcgpscostdiving/priority = -1002000

# frequency for calling primal heuristic <gcgpscostdiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/gcgpscostdiving/freq = 10

# frequency offset for calling primal heuristic <gcgpscostdiving>
# [type: int, range: [0,2147483647], default: 2]
heuristics/gcgpscostdiving/freqofs = 2

# maximal depth level to call primal heuristic <gcgpscostdiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/gcgpscostdiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/gcgpscostdiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/gcgpscostdiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.05]
heuristics/gcgpscostdiving/maxlpiterquot = 0.05

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/gcgpscostdiving/maxlpiterofs = 1000

# maximal number of allowed pricing rounds (-1: no limit)
# [type: int, range: [-1,2147483647], default: 0]
heuristics/gcgpscostdiving/maxpricerounds = 0

# perform pricing only if infeasibility is encountered
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/gcgpscostdiving/usefarkasonly = TRUE

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/gcgpscostdiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/gcgpscostdiving/maxdiveavgquot = 0

# maximal UBQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1], default: 0.1]
heuristics/gcgpscostdiving/maxdiveubquotnosol = 0.1

# maximal AVGQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/gcgpscostdiving/maxdiveavgquotnosol = 0

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/gcgpscostdiving/backtrack = TRUE

# shall pseudocosts be calculated w.r.t. the master problem?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/gcgpscostdiving/usemasterpscosts = FALSE

# priority of heuristic <gcgveclendiving>
# [type: int, range: [-536870912,536870911], default: -1003100]
heuristics/gcgveclendiving/priority = -1003100

# frequency for calling primal heuristic <gcgveclendiving> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/gcgveclendiving/freq = 10

# frequency offset for calling primal heuristic <gcgveclendiving>
# [type: int, range: [0,2147483647], default: 4]
heuristics/gcgveclendiving/freqofs = 4

# maximal depth level to call primal heuristic <gcgveclendiving> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/gcgveclendiving/maxdepth = -1

# minimal relative depth to start diving
# [type: real, range: [0,1], default: 0]
heuristics/gcgveclendiving/minreldepth = 0

# maximal relative depth to start diving
# [type: real, range: [0,1], default: 1]
heuristics/gcgveclendiving/maxreldepth = 1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.05]
heuristics/gcgveclendiving/maxlpiterquot = 0.05

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/gcgveclendiving/maxlpiterofs = 1000

# maximal number of allowed pricing rounds (-1: no limit)
# [type: int, range: [-1,2147483647], default: 0]
heuristics/gcgveclendiving/maxpricerounds = 0

# perform pricing only if infeasibility is encountered
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/gcgveclendiving/usefarkasonly = TRUE

# maximal quotient (curlowerbound - lowerbound)/(cutoffbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1], default: 0.8]
heuristics/gcgveclendiving/maxdiveubquot = 0.8

# maximal quotient (curlowerbound - lowerbound)/(avglowerbound - lowerbound) where diving is performed (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/gcgveclendiving/maxdiveavgquot = 0

# maximal UBQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1], default: 0.1]
heuristics/gcgveclendiving/maxdiveubquotnosol = 0.1

# maximal AVGQUOT when no solution was found yet (0.0: no limit)
# [type: real, range: [0,1.79769313486232e+308], default: 0]
heuristics/gcgveclendiving/maxdiveavgquotnosol = 0

# use one level of backtracking if infeasibility is encountered?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/gcgveclendiving/backtrack = TRUE

# calculate vector length scores w.r.t. the master LP?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/gcgveclendiving/usemasterscores = FALSE

# priority of heuristic <gcgdins>
# [type: int, range: [-536870912,536870911], default: -1105000]
heuristics/gcgdins/priority = -1105000

# frequency for calling primal heuristic <gcgdins> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/gcgdins/freq = -1

# frequency offset for calling primal heuristic <gcgdins>
# [type: int, range: [0,2147483647], default: 0]
heuristics/gcgdins/freqofs = 0

# maximal depth level to call primal heuristic <gcgdins> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/gcgdins/maxdepth = -1

# number of nodes added to the contingent of the total nodes
# [type: longint, range: [0,9223372036854775807], default: 5000]
heuristics/gcgdins/nodesofs = 5000

# contingent of sub problem nodes in relation to the number of nodes of the original problem
# [type: real, range: [0,1], default: 0.05]
heuristics/gcgdins/nodesquot = 0.05

# minimum number of nodes required to start the subproblem
# [type: longint, range: [0,9223372036854775807], default: 500]
heuristics/gcgdins/minnodes = 500

# number of pool-solutions to be checked for flag array update (for hard fixing of binary variables)
# [type: int, range: [1,2147483647], default: 5]
heuristics/gcgdins/solnum = 5

# radius (using Manhattan metric) of the incumbent's neighborhood to be searched
# [type: int, range: [1,2147483647], default: 18]
heuristics/gcgdins/neighborhoodsize = 18

# maximum number of nodes to regard in the subproblem
# [type: longint, range: [0,9223372036854775807], default: 5000]
heuristics/gcgdins/maxnodes = 5000

# factor by which gcgdins should at least improve the incumbent
# [type: real, range: [0,1], default: 0.01]
heuristics/gcgdins/minimprove = 0.01

# number of nodes without incumbent change that heuristic should wait
# [type: longint, range: [0,9223372036854775807], default: 0]
heuristics/gcgdins/nwaitingnodes = 0

# should subproblem be created out of the rows in the LP rows?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/gcgdins/uselprows = FALSE

# if uselprows == FALSE, should all active cuts from cutpool be copied to constraints in subproblem?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/gcgdins/copycuts = TRUE

# priority of heuristic <gcgfeaspump>
# [type: int, range: [-536870912,536870911], default: -1000000]
heuristics/gcgfeaspump/priority = -1000000

# frequency for calling primal heuristic <gcgfeaspump> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/gcgfeaspump/freq = -1

# frequency offset for calling primal heuristic <gcgfeaspump>
# [type: int, range: [0,2147483647], default: 0]
heuristics/gcgfeaspump/freqofs = 0

# maximal depth level to call primal heuristic <gcgfeaspump> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/gcgfeaspump/maxdepth = -1

# maximal fraction of diving LP iterations compared to node LP iterations
# [type: real, range: [0,1.79769313486232e+308], default: 0.01]
heuristics/gcgfeaspump/maxlpiterquot = 0.01

# factor by which the regard of the objective is decreased in each round, 1.0 for dynamic
# [type: real, range: [0,1], default: 1]
heuristics/gcgfeaspump/objfactor = 1

# threshold difference for the convex parameter to perform perturbation
# [type: real, range: [0,1], default: 1]
heuristics/gcgfeaspump/alphadiff = 1

# additional number of allowed LP iterations
# [type: int, range: [0,2147483647], default: 1000]
heuristics/gcgfeaspump/maxlpiterofs = 1000

# total number of feasible solutions found up to which heuristic is called (-1: no limit)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/gcgfeaspump/maxsols = 10

# maximal number of pumping loops (-1: no limit)
# [type: int, range: [-1,2147483647], default: 10000]
heuristics/gcgfeaspump/maxloops = 10000

# maximal number of pumping rounds without fractionality improvement (-1: no limit)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/gcgfeaspump/maxstallloops = 10

# minimum number of random variables to flip, if a 1-cycle is encountered
# [type: int, range: [1,2147483647], default: 10]
heuristics/gcgfeaspump/minflips = 10

# maximum length of cycles to be checked explicitly in each round
# [type: int, range: [1,100], default: 3]
heuristics/gcgfeaspump/cyclelength = 3

# number of iterations until a random perturbation is forced
# [type: int, range: [1,2147483647], default: 100]
heuristics/gcgfeaspump/perturbfreq = 100

# radius (using Manhattan metric) of the neighborhood to be searched in stage 3
# [type: int, range: [1,2147483647], default: 18]
heuristics/gcgfeaspump/neighborhoodsize = 18

# should an iterative round-and-propagate scheme be used to find the integral points?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/gcgfeaspump/usefp20 = FALSE

# should a random perturbation be performed if a feasible solution was found?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/gcgfeaspump/pertsolfound = TRUE

# should we solve a local branching sub-MIP if no solution could be found?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/gcgfeaspump/stage3 = FALSE

# should all active cuts from cutpool be copied to constraints in subproblem?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/gcgfeaspump/copycuts = TRUE

# priority of heuristic <gcgrens>
# [type: int, range: [-536870912,536870911], default: -1100000]
heuristics/gcgrens/priority = -1100000

# frequency for calling primal heuristic <gcgrens> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 0]
heuristics/gcgrens/freq = 0

# frequency offset for calling primal heuristic <gcgrens>
# [type: int, range: [0,2147483647], default: 0]
heuristics/gcgrens/freqofs = 0

# maximal depth level to call primal heuristic <gcgrens> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/gcgrens/maxdepth = -1

# minimum percentage of integer variables that have to be fixable
# [type: real, range: [0,1], default: 0.5]
heuristics/gcgrens/minfixingrate = 0.5

# maximum number of nodes to regard in the subproblem
# [type: longint, range: [0,9223372036854775807], default: 5000]
heuristics/gcgrens/maxnodes = 5000

# number of nodes added to the contingent of the total nodes
# [type: longint, range: [0,9223372036854775807], default: 500]
heuristics/gcgrens/nodesofs = 500

# minimum number of nodes required to start the subproblem
# [type: longint, range: [0,9223372036854775807], default: 500]
heuristics/gcgrens/minnodes = 500

# contingent of sub problem nodes in relation to the number of nodes of the original problem
# [type: real, range: [0,1], default: 0.1]
heuristics/gcgrens/nodesquot = 0.1

# factor by which RENS should at least improve the incumbent
# [type: real, range: [0,1], default: 0.01]
heuristics/gcgrens/minimprove = 0.01

# should general integers get binary bounds [floor(.),ceil(.)] ?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/gcgrens/binarybounds = TRUE

# should subproblem be created out of the rows in the LP rows?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/gcgrens/uselprows = FALSE

# if uselprows == FALSE, should all active cuts from cutpool be copied to constraints in subproblem?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/gcgrens/copycuts = TRUE

# should all subproblem solutions be added to the original SCIP?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/gcgrens/addallsols = FALSE

# priority of heuristic <gcgrins>
# [type: int, range: [-536870912,536870911], default: -1101000]
heuristics/gcgrins/priority = -1101000

# frequency for calling primal heuristic <gcgrins> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 20]
heuristics/gcgrins/freq = 20

# frequency offset for calling primal heuristic <gcgrins>
# [type: int, range: [0,2147483647], default: 5]
heuristics/gcgrins/freqofs = 5

# maximal depth level to call primal heuristic <gcgrins> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/gcgrins/maxdepth = -1

# number of nodes added to the contingent of the total nodes
# [type: int, range: [0,2147483647], default: 500]
heuristics/gcgrins/nodesofs = 500

# maximum number of nodes to regard in the subproblem
# [type: int, range: [0,2147483647], default: 5000]
heuristics/gcgrins/maxnodes = 5000

# minimum number of nodes required to start the subproblem
# [type: int, range: [0,2147483647], default: 500]
heuristics/gcgrins/minnodes = 500

# contingent of sub problem nodes in relation to the number of nodes of the original problem
# [type: real, range: [0,1], default: 0.1]
heuristics/gcgrins/nodesquot = 0.1

# number of nodes without incumbent change that heuristic should wait
# [type: int, range: [0,2147483647], default: 200]
heuristics/gcgrins/nwaitingnodes = 200

# factor by which gcgrins should at least improve the incumbent
# [type: real, range: [0,1], default: 0.01]
heuristics/gcgrins/minimprove = 0.01

# minimum percentage of integer variables that have to be fixed
# [type: real, range: [0,1], default: 0]
heuristics/gcgrins/minfixingrate = 0

# should subproblem be created out of the rows in the LP rows?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/gcgrins/uselprows = FALSE

# if uselprows == FALSE, should all active cuts from cutpool be copied to constraints in subproblem?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/gcgrins/copycuts = TRUE

# priority of heuristic <gcgrounding>
# [type: int, range: [-536870912,536870911], default: -1000]
heuristics/gcgrounding/priority = -1000

# frequency for calling primal heuristic <gcgrounding> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 1]
heuristics/gcgrounding/freq = 1

# frequency offset for calling primal heuristic <gcgrounding>
# [type: int, range: [0,2147483647], default: 0]
heuristics/gcgrounding/freqofs = 0

# maximal depth level to call primal heuristic <gcgrounding> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/gcgrounding/maxdepth = -1

# number of calls per found solution that are considered as standard success, a higher factor causes the heuristic to be called more often
# [type: int, range: [-1,2147483647], default: 100]
heuristics/gcgrounding/successfactor = 100

# priority of heuristic <gcgshifting>
# [type: int, range: [-536870912,536870911], default: -5000]
heuristics/gcgshifting/priority = -5000

# frequency for calling primal heuristic <gcgshifting> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 10]
heuristics/gcgshifting/freq = 10

# frequency offset for calling primal heuristic <gcgshifting>
# [type: int, range: [0,2147483647], default: 0]
heuristics/gcgshifting/freqofs = 0

# maximal depth level to call primal heuristic <gcgshifting> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/gcgshifting/maxdepth = -1

# priority of heuristic <gcgsimplerounding>
# [type: int, range: [-536870912,536870911], default: 0]
heuristics/gcgsimplerounding/priority = 0

# frequency for calling primal heuristic <gcgsimplerounding> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 1]
heuristics/gcgsimplerounding/freq = 1

# frequency offset for calling primal heuristic <gcgsimplerounding>
# [type: int, range: [0,2147483647], default: 0]
heuristics/gcgsimplerounding/freqofs = 0

# maximal depth level to call primal heuristic <gcgsimplerounding> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/gcgsimplerounding/maxdepth = -1

# priority of heuristic <gcgzirounding>
# [type: int, range: [-536870912,536870911], default: -500]
heuristics/gcgzirounding/priority = -500

# frequency for calling primal heuristic <gcgzirounding> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/gcgzirounding/freq = -1

# frequency offset for calling primal heuristic <gcgzirounding>
# [type: int, range: [0,2147483647], default: 0]
heuristics/gcgzirounding/freqofs = 0

# maximal depth level to call primal heuristic <gcgzirounding> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/gcgzirounding/maxdepth = -1

# determines maximum number of rounding loops
# [type: int, range: [0,2147483647], default: 2]
heuristics/gcgzirounding/maxroundingloops = 2

# flag to determine if Zirounding is deactivated after a certain percentage of unsuccessful calls
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/gcgzirounding/stopziround = TRUE

# if percentage of found solutions falls below this parameter, Zirounding will be deactivated
# [type: real, range: [0,1], default: 0.02]
heuristics/gcgzirounding/stoppercentage = 0.02

# determines the minimum number of calls before percentage-based deactivation of Zirounding is applied
# [type: int, range: [1,2147483647], default: 1000]
heuristics/gcgzirounding/minstopncalls = 1000

# priority of heuristic <xpcrossover>
# [type: int, range: [-536870912,536870911], default: -1100500]
heuristics/xpcrossover/priority = -1100500

# frequency for calling primal heuristic <xpcrossover> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 0]
heuristics/xpcrossover/freq = 0

# frequency offset for calling primal heuristic <xpcrossover>
# [type: int, range: [0,2147483647], default: 0]
heuristics/xpcrossover/freqofs = 0

# maximal depth level to call primal heuristic <xpcrossover> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/xpcrossover/maxdepth = -1

# number of nodes added to the contingent of the total nodes
# [type: longint, range: [0,9223372036854775807], default: 200]
heuristics/xpcrossover/nodesofs = 200

# maximum number of nodes to regard in the subproblem
# [type: longint, range: [0,9223372036854775807], default: 1000]
heuristics/xpcrossover/maxnodes = 1000

# minimum number of nodes required to start the subproblem
# [type: longint, range: [0,9223372036854775807], default: 200]
heuristics/xpcrossover/minnodes = 200

# number of extreme pts per block that will be taken into account
# [type: int, range: [2,2147483647], default: 4]
heuristics/xpcrossover/nusedpts = 4

# number of nodes without incumbent change that heuristic should wait
# [type: longint, range: [0,9223372036854775807], default: 200]
heuristics/xpcrossover/nwaitingnodes = 200

# contingent of sub problem nodes in relation to the number of nodes of the original problem
# [type: real, range: [0,1], default: 0.1]
heuristics/xpcrossover/nodesquot = 0.1

# minimum percentage of integer variables that have to be fixed
# [type: real, range: [0,1], default: 0.4]
heuristics/xpcrossover/minfixingrate = 0.4

# factor by which crossover should at least improve the incumbent
# [type: real, range: [0,1], default: 0.01]
heuristics/xpcrossover/minimprove = 0.01

# should the choice which sols to take be randomized?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/xpcrossover/randomization = FALSE

# should the nwaitingnodes parameter be ignored at the root node?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/xpcrossover/dontwaitatroot = FALSE

# should subproblem be created out of the rows in the LP rows?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/xpcrossover/uselprows = FALSE

# if uselprows == FALSE, should all active cuts from cutpool be copied to constraints in subproblem?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/xpcrossover/copycuts = TRUE

# priority of heuristic <xprins>
# [type: int, range: [-536870912,536870911], default: -1100600]
heuristics/xprins/priority = -1100600

# frequency for calling primal heuristic <xprins> (-1: never, 0: only at depth freqofs)
# [type: int, range: [-1,2147483647], default: 0]
heuristics/xprins/freq = 0

# frequency offset for calling primal heuristic <xprins>
# [type: int, range: [0,2147483647], default: 0]
heuristics/xprins/freqofs = 0

# maximal depth level to call primal heuristic <xprins> (-1: no limit)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/xprins/maxdepth = -1

# minimum percentage of coincidence of relaxation and extreme pts
# [type: real, range: [0,1], default: 0.5]
heuristics/xprins/equalityrate = 0.5

# number of nodes added to the contingent of the total nodes
# [type: longint, range: [0,9223372036854775807], default: 200]
heuristics/xprins/nodesofs = 200

# maximum number of nodes to regard in the subproblem
# [type: longint, range: [0,9223372036854775807], default: 1000]
heuristics/xprins/maxnodes = 1000

# minimum number of nodes required to start the subproblem
# [type: longint, range: [0,9223372036854775807], default: 200]
heuristics/xprins/minnodes = 200

# number of extreme pts per block that will be taken into account (-1: all; 0: all which contribute to current relaxation solution)
# [type: int, range: [-1,2147483647], default: -1]
heuristics/xprins/nusedpts = -1

# number of nodes without incumbent change that heuristic should wait
# [type: longint, range: [0,9223372036854775807], default: 200]
heuristics/xprins/nwaitingnodes = 200

# contingent of sub problem nodes in relation to the number of nodes of the original problem
# [type: real, range: [0,1], default: 0.1]
heuristics/xprins/nodesquot = 0.1

# minimum percentage of integer variables that have to be fixed
# [type: real, range: [0,1], default: 0.4]
heuristics/xprins/minfixingrate = 0.4

# factor by which crossover should at least improve the incumbent
# [type: real, range: [0,1], default: 0.01]
heuristics/xprins/minimprove = 0.01

# should the choice which sols to take be randomized?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/xprins/randomization = FALSE

# should the nwaitingnodes parameter be ignored at the root node?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/xprins/dontwaitatroot = FALSE

# should subproblem be created out of the rows in the LP rows?
# [type: bool, range: {TRUE,FALSE}, default: FALSE]
heuristics/xprins/uselprows = FALSE

# if uselprows == FALSE, should all active cuts from cutpool be copied to constraints in subproblem?
# [type: bool, range: {TRUE,FALSE}, default: TRUE]
heuristics/xprins/copycuts = TRUE

# display activation status of display column <solfound> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/solfound/active = 1

# display activation status of display column <time> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/time/active = 1

# display activation status of display column <nnodes> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/nnodes/active = 1

# display activation status of display column <nodesleft> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/nodesleft/active = 1

# display activation status of display column <lpiterations> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/lpiterations/active = 1

# display activation status of display column <lpavgiterations> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/lpavgiterations/active = 1

# display activation status of display column <lpcond> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/lpcond/active = 1

# display activation status of display column <memused> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/memused/active = 1

# display activation status of display column <depth> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/depth/active = 1

# display activation status of display column <maxdepth> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/maxdepth/active = 1

# display activation status of display column <plungedepth> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/plungedepth/active = 1

# display activation status of display column <nfrac> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/nfrac/active = 1

# display activation status of display column <nexternbranchcands> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/nexternbranchcands/active = 1

# display activation status of display column <vars> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/vars/active = 1

# display activation status of display column <conss> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/conss/active = 1

# display activation status of display column <curconss> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/curconss/active = 1

# display activation status of display column <curcols> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/curcols/active = 1

# display activation status of display column <currows> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/currows/active = 1

# display activation status of display column <cuts> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/cuts/active = 1

# display activation status of display column <separounds> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/separounds/active = 1

# display activation status of display column <poolsize> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/poolsize/active = 1

# display activation status of display column <conflicts> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/conflicts/active = 1

# display activation status of display column <strongbranchs> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/strongbranchs/active = 1

# display activation status of display column <pseudoobj> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/pseudoobj/active = 1

# display activation status of display column <lpobj> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/lpobj/active = 1

# display activation status of display column <curdualbound> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/curdualbound/active = 1

# display activation status of display column <estimate> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/estimate/active = 1

# display activation status of display column <avgdualbound> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/avgdualbound/active = 1

# display activation status of display column <dualbound> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/dualbound/active = 1

# display activation status of display column <primalbound> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/primalbound/active = 1

# display activation status of display column <cutoffbound> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/cutoffbound/active = 1

# display activation status of display column <gap> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/gap/active = 1

# display activation status of display column <primalgap> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 0]
display/primalgap/active = 0

# display activation status of display column <nsols> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/nsols/active = 1

# display activation status of display column <mlpiterations> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/mlpiterations/active = 1

# display activation status of display column <mvars> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/mvars/active = 1

# display activation status of display column <mconss> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/mconss/active = 1

# display activation status of display column <mcuts> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/mcuts/active = 1

# display activation status of display column <degeneracy> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/degeneracy/active = 1

# display activation status of display column <sumlpiterations> (0: off, 1: auto, 2:on)
# [type: int, range: [0,2], default: 1]
display/sumlpiterations/active = 1
